<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 7 Correlation and Linear Regression | Applied Statistics in Environment and Ecology</title>
  <meta name="description" content="A book to support content in ERS 669." />
  <meta name="generator" content="bookdown 0.37 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 7 Correlation and Linear Regression | Applied Statistics in Environment and Ecology" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="A book to support content in ERS 669." />
  <meta name="github-repo" content="openscapes/series" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 7 Correlation and Linear Regression | Applied Statistics in Environment and Ecology" />
  
  <meta name="twitter:description" content="A book to support content in ERS 669." />
  

<meta name="author" content="Dr.Â Brad Fedy" />


<meta name="date" content="2024-02-10" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="data-exploration.html"/>

<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="lib/css/bootstrap.min.css" type="text/css" />
<link rel="stylesheet" href="lib/css/style.css" type="text/css" />
<link rel="stylesheet" href="lib/css/lesson.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Welcome</a></li>
<li class="chapter" data-level="2" data-path="introduction-to-r.html"><a href="introduction-to-r.html"><i class="fa fa-check"></i><b>2</b> Introduction to R</a>
<ul>
<li class="chapter" data-level="2.1" data-path="introduction-to-r.html"><a href="introduction-to-r.html#what-is-r"><i class="fa fa-check"></i><b>2.1</b> What is R?</a></li>
<li class="chapter" data-level="2.2" data-path="introduction-to-r.html"><a href="introduction-to-r.html#rstudio"><i class="fa fa-check"></i><b>2.2</b> RStudio</a></li>
<li class="chapter" data-level="2.3" data-path="introduction-to-r.html"><a href="introduction-to-r.html#before-you-begin"><i class="fa fa-check"></i><b>2.3</b> Before you begin</a></li>
<li class="chapter" data-level="2.4" data-path="introduction-to-r.html"><a href="introduction-to-r.html#setting-a-working-directory"><i class="fa fa-check"></i><b>2.4</b> Setting a working directory</a></li>
<li class="chapter" data-level="2.5" data-path="introduction-to-r.html"><a href="introduction-to-r.html#getting-help-in-r"><i class="fa fa-check"></i><b>2.5</b> Getting help in R</a></li>
<li class="chapter" data-level="2.6" data-path="introduction-to-r.html"><a href="introduction-to-r.html#defining-r-objects"><i class="fa fa-check"></i><b>2.6</b> Defining R objects</a></li>
<li class="chapter" data-level="2.7" data-path="introduction-to-r.html"><a href="introduction-to-r.html#exporting-data-frames"><i class="fa fa-check"></i><b>2.7</b> Exporting data frames</a>
<ul>
<li class="chapter" data-level="2.7.1" data-path="introduction-to-r.html"><a href="introduction-to-r.html#installing-and-loading-packages"><i class="fa fa-check"></i><b>2.7.1</b> Installing and loading packages</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="operators.html"><a href="operators.html"><i class="fa fa-check"></i><b>3</b> Operators</a>
<ul>
<li class="chapter" data-level="3.1" data-path="operators.html"><a href="operators.html#manipulating-r-objects"><i class="fa fa-check"></i><b>3.1</b> Manipulating R objects</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="data.html"><a href="data.html"><i class="fa fa-check"></i><b>4</b> Data</a>
<ul>
<li class="chapter" data-level="4.1" data-path="data.html"><a href="data.html#writing-scripts"><i class="fa fa-check"></i><b>4.1</b> Writing scripts</a></li>
<li class="chapter" data-level="4.2" data-path="data.html"><a href="data.html#importing-viewing-and-editing-data"><i class="fa fa-check"></i><b>4.2</b> Importing, viewing, and editing data</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="data.html"><a href="data.html#summarizing-and-manipulating-data"><i class="fa fa-check"></i><b>4.2.1</b> Summarizing and manipulating data</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="data-visualization.html"><a href="data-visualization.html"><i class="fa fa-check"></i><b>5</b> Data Visualization</a>
<ul>
<li class="chapter" data-level="5.1" data-path="data-visualization.html"><a href="data-visualization.html#plotting-data"><i class="fa fa-check"></i><b>5.1</b> Plotting data</a></li>
<li class="chapter" data-level="5.2" data-path="data-visualization.html"><a href="data-visualization.html#manipulating-plots"><i class="fa fa-check"></i><b>5.2</b> Manipulating plots</a></li>
<li class="chapter" data-level="5.3" data-path="data-visualization.html"><a href="data-visualization.html#adding-low-level-plotting-functions"><i class="fa fa-check"></i><b>5.3</b> Adding low-level plotting functions</a></li>
<li class="chapter" data-level="5.4" data-path="data-visualization.html"><a href="data-visualization.html#facet-plots"><i class="fa fa-check"></i><b>5.4</b> Facet plots</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="data-exploration.html"><a href="data-exploration.html"><i class="fa fa-check"></i><b>6</b> Data Exploration</a>
<ul>
<li class="chapter" data-level="6.1" data-path="data-exploration.html"><a href="data-exploration.html#outliers"><i class="fa fa-check"></i><b>6.1</b> Outliers</a></li>
<li class="chapter" data-level="6.2" data-path="data-exploration.html"><a href="data-exploration.html#homoscedasticity-homegeneity-of-variance"><i class="fa fa-check"></i><b>6.2</b> Homoscedasticity (homegeneity of variance)</a></li>
<li class="chapter" data-level="6.3" data-path="data-exploration.html"><a href="data-exploration.html#distribution-of-the-response-variable"><i class="fa fa-check"></i><b>6.3</b> Distribution of the response variable</a></li>
<li class="chapter" data-level="6.4" data-path="data-exploration.html"><a href="data-exploration.html#collinearity-among-x-variables"><i class="fa fa-check"></i><b>6.4</b> Collinearity among x variables</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="correlation-and-linear-regression.html"><a href="correlation-and-linear-regression.html"><i class="fa fa-check"></i><b>7</b> Correlation and Linear Regression</a>
<ul>
<li class="chapter" data-level="7.1" data-path="correlation-and-linear-regression.html"><a href="correlation-and-linear-regression.html#correlation"><i class="fa fa-check"></i><b>7.1</b> Correlation</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="correlation-and-linear-regression.html"><a href="correlation-and-linear-regression.html#correlation-coefficient"><i class="fa fa-check"></i><b>7.1.1</b> Correlation coefficient</a></li>
<li class="chapter" data-level="7.1.2" data-path="correlation-and-linear-regression.html"><a href="correlation-and-linear-regression.html#non-parametric-correlations"><i class="fa fa-check"></i><b>7.1.2</b> Non-parametric correlations</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="correlation-and-linear-regression.html"><a href="correlation-and-linear-regression.html#regression"><i class="fa fa-check"></i><b>7.2</b> Regression</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="correlation-and-linear-regression.html"><a href="correlation-and-linear-regression.html#interpreting-output"><i class="fa fa-check"></i><b>7.2.1</b> Interpreting Output</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="correlation-and-linear-regression.html"><a href="correlation-and-linear-regression.html#model-validation"><i class="fa fa-check"></i><b>7.3</b> Model Validation</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Applied Statistics in Environment and Ecology</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="correlation-and-linear-regression" class="section level1 hasAnchor" number="7">
<h1><span class="header-section-number">Chapter 7</span> Correlation and Linear Regression<a href="correlation-and-linear-regression.html#correlation-and-linear-regression" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="correlation" class="section level2 hasAnchor" number="7.1">
<h2><span class="header-section-number">7.1</span> Correlation<a href="correlation-and-linear-regression.html#correlation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="correlation-coefficient" class="section level3 hasAnchor" number="7.1.1">
<h3><span class="header-section-number">7.1.1</span> Correlation coefficient<a href="correlation-and-linear-regression.html#correlation-coefficient" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We addressed correlation with some code and examples provided in the Data Exploration chapter. This section will provide a few more details on correlation estimation and how to explore correlations with non-parametric data.</p>
<p>Correlation is the most basic means of measuring the association between two variables. Pearsonâs correlation (<em>r</em>) is used to estimate the level of correlation among two continuous and normally-distributed variables and ranges from -1 to 1. Perhaps too obvious for comment, but values &lt; 0 indicated a negative relationship, values &gt; 0 indicate a positive relationship with 0 indicating no relationship. There is no test of <em>significance</em> <em>per se</em> for the Pearson. However, there are some general guidelines. Generally speaking, <em>r</em> &lt; |0.3| is considered weak, moderate values are |0.3| &lt; <em>r</em> &lt; |0.7|, and an <em>r</em> &gt; |0.7| is considered high correlation. The â|â around the numbers indicate they can take on a negative or positive value.</p>
</div>
<div id="non-parametric-correlations" class="section level3 hasAnchor" number="7.1.2">
<h3><span class="header-section-number">7.1.2</span> Non-parametric correlations<a href="correlation-and-linear-regression.html#non-parametric-correlations" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Often times data are not normally distributed or continuous. Luckily, a series of other statistics have been developed to measure the relationships among variable with different measurement scales. After Pearsonâs correlation coefficient, the next most common approach in ecology and environment is likely the Spearmanâs rank correlation. Spearmanâs correlates two variables measured on the ordinal scale. The raw data may have actually been measured on an ordinal scale or if data are continuous, but not normally distributed, the researcher may decide to convert the variable to the ordinal scale and assess correlation using a Spearmanâs rank correlation. This type of transformation from a non-normal continuous variable to a rank-ordered variable is a common approach in non-parametric statistics (<em>e.g.</em>, Wilcoxon, Mann-Whitney U-test) and we encounter it again when dealing with GLMs. An alternative to Spearmanâs that you may encounter is Kendallâs tau, which is an alternative test of association between oridinal data. My experience suggests it is less common in the ecology and environment literature.</p>
<p>Categorical data provide a different challenge and associations (<em>i.e.</em>, correlations) are commonly assessed using the Pearson Chi-squared test for independence. With this type of data we are testing an explicit hypothesis. The null hypothesis (<span class="math inline">\(H_0\)</span>): no association between the two variables, is compared against the alternative hypothesis (<span class="math inline">\(H_A\)</span>): there is an association between the two variables. The Chi-squared test results in a significance test which can reject the null hypothesis. Cramerâs V is an alternative to the Chi-squared test. Cramerâs V does not reject or fail to reject a hypothesis and is more similar to the Pearsonâs correlation coefficient in its interpretation producing a value between 0 (no association) and 1 (perfect association). Guidelines suggest that a Cramerâs V of &lt; 0.25 indicates a week association and &gt; 0.75 is a moderate association.</p>
<p>All the above approaches assume that individual observations are drawn randomly from the population.</p>
</div>
</div>
<div id="regression" class="section level2 hasAnchor" number="7.2">
<h2><span class="header-section-number">7.2</span> Regression<a href="correlation-and-linear-regression.html#regression" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The simplest linear models we can develop are linear regression models. These models are similar to the concept of correlation introduced in the previous section in that we are only examining two variables. However, linear regression differs from correlation in several important ways. Conceptually, the biggest difference, is that in a linear regression you are proposing a <em>directional</em> relationship. When we looked at correlations, we were simply interested in whether there was a relationship between two variables, but we did not propose a cause and effect relationship. In other words, in correlation, it does not matter what variable is on the <span class="math inline">\(y\)</span> axis and what variable was on the <span class="math inline">\(x\)</span> axis. The variables are interchangeable between axes. This is not true of a linear regression. For a linear regression, we must specify which of the variables we are trying to predict (i.e., which variable is the <em>dependent</em> variable. This variable is the <span class="math inline">\(y\)</span> variable. The predictor (i.e., <em>independent</em> variable) is the <span class="math inline">\(x\)</span> variable. Our selection of dependent <span class="math inline">\(y\)</span> and independent <span class="math inline">\(x\)</span> variables is informed by our question and the hypotheses that we want to test. Essentially, the <span class="math inline">\(y\)</span> is what you want to predict - your response variable of interest - and your <span class="math inline">\(x\)</span> is something that influences that response. We will go through many examples throughout the book.</p>
<p>These models form the basis for all subsequent and more sophisticated modelling that we will explore in this book. Essentially, a simple linear regression models a linear (i.e., straight line) relationship between a response variable <span class="math inline">\(y\)</span> with a normal distribution and a single, numerical, explanatory variable. We introduced the linear model in the past chapter. A simple linear regression is evenâ¦ wellâ¦ simpler and can be represented by the following equation.</p>
<p><span class="math display">\[y_i = \beta_0 + \beta_1x_1 + \epsilon_i\]</span></p>
<p>Another common way of representing this equation involves presenting more detail about the shape of the response variable. In this case, we specify the response variable has a normal distribution (<span class="math inline">\(Y\)</span> ~ Normal(<span class="math inline">\(\mu_Y, \sigma^2_Y\)</span>)) and therefore our model is written as:</p>
<p><span class="math display">\[\mu_Y = \beta_0 + \beta_1x_1 + \epsilon_i\]</span></p>
<p>The <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1x_1\)</span> have the same interpretation. As a reminder, <span class="math inline">\(\beta_0\)</span> is the intercept (i.e., where the line crosses the <span class="math inline">\(y\)</span> axis). Or, in other words, <span class="math inline">\(\beta_0\)</span> is the predicted value of <span class="math inline">\(\mu_y\)</span> when <span class="math inline">\(x\)</span> = 0. The slope of the straight line is <span class="math inline">\(\beta_1\)</span>. I once had a highly respected and intelligent professor tell me that, in ecology, âit is all about the <em>Betas</em>â. This slope can be presented as the rate of change in <span class="math inline">\(\mu_Y\)</span> per unit change in the explanatory variable <span class="math inline">\(x_1\)</span>.</p>
<p>Throughout the book, I have tried to use ârealâ ecological data collected by me and my colleagues. I think this is important because the data are messy and frequently do not behave well. These data can have challenging distributions that can make fitting models to the data difficult. Sometimes wrangling ârealâ world ecological data and fitting models to those data is as frustrating and challenging as stuffing a tired and angry toddler into a snow suitâ¦ in a hot roomâ¦ while sleep deprivedâ¦ with your mother-in-law explaining how you are going to be late while she muses that she never had such problems with her kids. But, working with those data will help you with your own data. However, while digging through all the many datasets I have compiled across my career, it turns out I have never collected data that could be modeled using a simple linear regression. Lucky me. So the following data and examples are from the excellent textbook by Pablo Inchausti, <a href="https://global.oup.com/academic/product/statistical-modeling-with-r-9780192859013?cc=us&amp;lang=en&amp;">Statistical Modeling with R</a>, which is currently available through the UW library for this course. The authorâs website with data, code, and extra material can be found <a href="https://sites.google.com/view/statistical-modeling-with-r/home">here</a>.</p>
<p>The data we will work with look at plant tolerance and cadmium levels. Cadmium (Cd) is a metal that has toxic effects on plant and human health. The researchers grew grass in soils with different concentrations of Cd to determine if the grass was a bioaccumulator of the metal to assess its usefulness in bioremediation. More details can be found in the textbook.</p>
<p>First, we will make sure the packages required are loaded. Remember, you must make sure they are installed.</p>
<div class="sourceCode" id="cb93"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb93-1"><a href="correlation-and-linear-regression.html#cb93-1" tabindex="-1"></a>packages.needed <span class="ot">&lt;-</span> <span class="fu">c</span>(<span class="st">&quot;ggplot2&quot;</span>, <span class="st">&quot;broom&quot;</span>, <span class="st">&quot;qqplotr&quot;</span>, <span class="st">&quot;cowplot&quot;</span>, <span class="st">&quot;ggeffects&quot;</span>)</span>
<span id="cb93-2"><a href="correlation-and-linear-regression.html#cb93-2" tabindex="-1"></a><span class="fu">lapply</span>(packages.needed, <span class="at">FUN =</span> require, <span class="at">character.only =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<p>Next we load the data provided by Inchausti. The data can be downloaded from the authorâs website (referenced above) or the UW LEARN site.</p>
<div class="sourceCode" id="cb94"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb94-1"><a href="correlation-and-linear-regression.html#cb94-1" tabindex="-1"></a>df <span class="ot">=</span> <span class="fu">read.csv</span>(<span class="st">&quot;DATA/Ch 04 Cadmium.csv&quot;</span>, <span class="at">header =</span> <span class="cn">TRUE</span>)</span>
<span id="cb94-2"><a href="correlation-and-linear-regression.html#cb94-2" tabindex="-1"></a><span class="fu">summary</span> (df)</span></code></pre></div>
<pre><code>##       soil         shoot            root     
##  Min.   : 60   Min.   : 16.2   Min.   : 105  
##  1st Qu.:120   1st Qu.: 52.6   1st Qu.: 245  
##  Median :180   Median :123.7   Median : 465  
##  Mean   :180   Mean   :117.0   Mean   : 503  
##  3rd Qu.:240   3rd Qu.:171.2   3rd Qu.: 665  
##  Max.   :300   Max.   :217.7   Max.   :1067</code></pre>
<p>The soil variable represents the level of Cd in the soil and the shoot variable represents the amount of Cd in the shoots.</p>
<p>The model we want to fit is: <span class="math display">\[shoot = \beta_0 + \beta_1soil + \epsilon_1\]</span></p>
<p>The <span class="math inline">\(\epsilon\)</span> represents the residuals and there are explicit assumptions about its distribution also. In this case we assume the residuals will follow a normal distribution with a mean = 0.</p>
<p>Fitting a simple linear regression in R uses the <code>lm</code> function which is part of the <code>stats</code> package. You do not need to load this package because it is automatically loaded by default at the start of every R session. We fit the linear model to our data using the following:</p>
<div class="sourceCode" id="cb96"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb96-1"><a href="correlation-and-linear-regression.html#cb96-1" tabindex="-1"></a>m1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(shoot <span class="sc">~</span> soil, <span class="at">data =</span> df)</span></code></pre></div>
<p>The code above fits the model we specified in the equation above. The shoot response variable and soil predictor covariate are present in the code. However, we do not need to specify any of the model-estimated components such as the intercept <span class="math inline">\(\beta_0\)</span> the slope of the relationship <span class="math inline">\(\beta_1\)</span> or the error term <span class="math inline">\(\epsilon_1\)</span> because these are unknown to us and are produced by the model estimating function <code>lm</code>. The tilde sign <code>~</code> in R fills the place of the <code>=</code> in the equation and separates the response and predictor variables. We also must specify the data frame which contains the variables with the <code>data = df</code> code. We store the results by assigning the model to an object (<code>m1</code> in the example above) which creates a list containing model results. The most relevant model results are provided using the <code>summary</code> command.</p>
<div class="sourceCode" id="cb97"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb97-1"><a href="correlation-and-linear-regression.html#cb97-1" tabindex="-1"></a><span class="fu">summary</span>(m1)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = shoot ~ soil, data = df)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -25.97 -11.22   1.73   7.66  28.68 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  -19.030      8.355   -2.28    0.035 *  
## soil           0.756      0.042   18.00  5.9e-13 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 15.9 on 18 degrees of freedom
## Multiple R-squared:  0.947,  Adjusted R-squared:  0.944 
## F-statistic:  324 on 1 and 18 DF,  p-value: 5.88e-13</code></pre>
<div id="interpreting-output" class="section level3 hasAnchor" number="7.2.1">
<h3><span class="header-section-number">7.2.1</span> Interpreting Output<a href="correlation-and-linear-regression.html#interpreting-output" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The most important model estimates are presented under the <code>Coefficients</code> heading (remember âItâs all about the Betasâ). The summary output presents estimates of the intercept (<span class="math inline">\(\beta_0\)</span>), the slope (<span class="math inline">\(\beta_1soil\)</span>), and the standard errors associated with each estimate (i.e., the variation around the estimate). The <code>Coefficients</code> section also presents the <code>t value</code> and the <em>p</em>-value (<code>Pr(&gt;|t|)</code>).</p>
<p>Recall from lecture, that within this frequentist paradigm all of these parameters are estimated using a maximum likelihood approach.</p>
<p>The <em>p</em>-value is calculated based on the <em>t</em>-distribution which is a type of normal distribution, centered on zero, in which the variance is estimated based on the <em>degrees of freedom</em>. The degrees of freedom are related to your sample size. Increasing the sample size <em>decreases</em> the variance, essentially <em>narrowing</em> your normal distribution. The <em>p</em>-value presented in the output represents the probability of obtaining a <em>t</em>-value equal to or more extreme than the one obtained if the null hypothesis is true (<em>i.e.</em> <span class="math inline">\(\beta_1\)</span> = 0). Therefore, the <em>p</em>-value essentially measures the lack of congruence between the data and the null hypothesis. In a frequentist context, the <em>p</em>-value essentially answers the question of how frequently we would observe the data if we ran this <em>experiment</em> many times. Data that deviate from the null hypothesis, based on model assumptions, have lower <em>p</em>-values. We will work through a simulation exercise in class that will hopefully be illuminative. In order to determine whether the results are actually important we must interpret the magnitude of the estimated slope and its error in the context of the ecology and research questions that motivated the data collection. In the example above, that means we need to decide if the absorption of 75.5% of the soil Cd by plants is important. The estimated intercept <span class="math inline">\(\beta_0\)</span> of -19.03 is largely meaningless in the context of this model because it estimates the average shoot concentration when there is no Cd in the soil (<em>i.e.</em> Cd = 0). Later we will discuss the possibility of standardizing your predictor variables and how that affects our interpretation of the intercept in linear models.</p>
<p>The bottom of the output presents several measures of the model performance. Each model can be used to generate predicted values (<em>i.e.</em>, the value of the response variable if the model is true). The <code>Residual standar error</code> essentially represents difference between observed and predicted values (<em>i.e</em>, the estimated variance of the residuals or unexplained variance in the model). The <code>Multiple R-squared</code> and the <code>Adjusted R-squared</code> values are the proportion of variation in the response variable that is explained by the model. You can find plenty of mathematical descriptions of the difference between these two metrics online, if you are interested. The main difference is that <code>Multiple R-squared</code> will stay the same or increase as you add new predictors (<em>i.e.</em>, covariates) to the model. This is true even if the new predictors are uninformative. Whereas <code>Adjusted R-squared</code> only increases if the new predictors improve the models predictive power and will decrease if the predictors are irrelevant to the models predictive performance. These values are both a measure of the goodness of fit of the model to the data. Higher values represent a better fit to the data, but should largely only be considered in a comparative context (<em>i.e.</em>, in relation to other models fitted to the same data).</p>
<p>The <code>Residuals:</code> section at the top of the model output is largely irrelevant to model interpretation.</p>
<p>Statistics is so relevant to the real world because it allows us to estimate the inherent uncertainty underlying any complex problem. The <code>Std. Error</code> component give us some indication of the variability around the estimated parameter, but it is typically considered a fairly <em>narrow</em> or overly optimistic estimate of our confidence in the estimate. Therefore, it is common to estimate the confidence interval surrounding the parameter estimate. The interpretation of a confidence interval in a frequentist context is different than in a Bayesian context. In a frequentist context, the confidence interval is a range of equally plausible values of the parameter that we could obtain if we repeated this study many times with the same sample size and the same population. As an aside, in a Bayesian context we would estimate a 95% credible interval which would be accurately interpreted as: there is a 95% chance the true value is within the range presented. This is different.</p>
<p>To obtain the confidence interval for the model <code>m1</code> we use the <code>confint( )</code> function.</p>
<div class="sourceCode" id="cb99"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb99-1"><a href="correlation-and-linear-regression.html#cb99-1" tabindex="-1"></a><span class="fu">confint</span>(m1)</span></code></pre></div>
<pre><code>##               2.5 % 97.5 %
## (Intercept) -36.584 -1.476
## soil          0.668  0.844</code></pre>
<p>So, what is the best way to interpret confidence intervals in the frequentist context? Consider it as a thought experiment. If we ran this study over and over again in the same population with the same sample size, we would end up with a distribution of parameter estimates with a lower 2.5% bound of 0.668 and an upper 97.5% bound of 0.844. Hopefully the example in class helps to further clarify this concept.</p>
</div>
</div>
<div id="model-validation" class="section level2 hasAnchor" number="7.3">
<h2><span class="header-section-number">7.3</span> Model Validation<a href="correlation-and-linear-regression.html#model-validation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>It is not enough to fit a model and simply present the parameter estimates. We also need to determine if it is a âgoodâ model. In other words, does the model fit the data well? The residuals and their distribution are the main focus of model validation. Essentially, the smaller the residuals (i.e., the difference between the observed and predicted values) the better the model fits the data. In addition to the basics of minimizing <span class="math inline">\(Y_{obs} - Y_{pred}\)</span> we also want simple and random patterns in the distribution of our residuals. Any trends in the residuals can be an indication of poor model fit. With a general linear model, the default model validation summary produces four graphs. You can produce these in base R with the <code>plot(model_name)</code> function. However, for greater flexibility and insight we will produce validation figures using ggplot. This approach is also used throughout most of the examples in Inchaustiâs textbook.</p>
<p>The first graph plots the Residuals vs.Â the Fitted values. The residuals are estimated for each of the observed values and are presented in the <em>y</em>-axis. Remember the residuals are estimated as <span class="math inline">\(Y_{obs} - Y_{pred}\)</span> and can therefore take on positive and negative values that <em>should</em> be distributed randomly around 0. The fitted values represent the model predicted values,encompass the range of your observed data, and are plotted on the <em>x</em>-axis. If the model fits the data well, we should see an even distribution of points scattered on both sides of the 0 value. There should be no visible pattern to the distributed values (<em>i.e.</em>, cone shaped). This implies a homogeneous distribution of the residuals, which is an informal assessment of homoscedacity (<em>i.e.</em>, homogeneity of variance).</p>
<p>A parallel plot to the Residuals vs.Â Fitted values, replaces the <em>x</em>-axis fitted values with the actual observed values and should have a similar pattern.</p>
<p>The third common plot presents the sample quantiles on the <em>y</em>-axis and the theoretical quantiles on the <em>x</em>-axis. Since this plot presents the two quantiles and is used to assess the ânormalityâ of the data, it is often refereed to as the âNormal Q-Q plotâ. Quantiles are points in your data below which a certain proportion of your data fall. In a normal distribution with a mean of 0, the 0.5 quantile (<em>i.e.</em>, 50th percentile) is 0. The quantiles presented are basically your data sorted in ascending order with each data point labelled as the point below which a certain proportion of the data fall. Therefore, if the model predicted (Theoretical quantiles) match the observed (Sample quantiles) perfectly, they will lie along a perfect diagonal. Thus,if the residuals are normally distributed, they should have a relatively tight scatter of points along the line of perfect fit. It is very common for the points near the tail ends of the distribution (upper and lower) to deviate more from the theoretical perfect fit than the points closer to the center for the distribution. This makes sense because there are more data to inform model fit in the central regions of the distribution than near the tails.</p>
<p>The final common model validation plot presents Cookâs distance. This is essentially a sensitivity analysis that quantifies the impact of each data point on the parameter estimates. Data points with a âlargeâ Cookâs distance may have an unduly large influence on the parameter estimates. There is no hard-and-fast rule to decide when a Cookâs distance is âtooâ large. Like much of model validation, it is somewhat subjective and relies on our visual interpretation of the validation plots. If you are concerned about the influence of a particular observation, you can remove that observation, re-run the model and look at the changes in the model parameter estimates. The Cookâs distance plot presents the observations in your data set in order along the <em>x</em>-axis so it is easy to identify potentially large observations. The <em>y</em>-axis presents the Cookâs distance. If you are interested, I will leave you to look up the actual calculation of Cookâs distance on your own. It is not necessary to understand the underlying mathematics in detail. It is enough to know what it represents and how it can inform your modelling choices and interpretation.</p>
<p>Producing ggplot-type figures requires use of the package <code>broom</code>. First, we create a tibble.</p>
<div class="sourceCode" id="cb101"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb101-1"><a href="correlation-and-linear-regression.html#cb101-1" tabindex="-1"></a><span class="fu">tidy</span>(m1, <span class="at">conf.int=</span><span class="cn">TRUE</span>)</span></code></pre></div>
<pre><code>## # A tibble: 2 Ã 7
##   term        estimate std.error statistic  p.value conf.low conf.high
##   &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;
## 1 (Intercept)  -19.0      8.36       -2.28 3.52e- 2  -36.6      -1.48 
## 2 soil           0.756    0.0420     18.0  5.88e-13    0.668     0.844</code></pre>
<p>The <code>glance</code> function gathers the model fit data into a more user-friendly format.</p>
<div class="sourceCode" id="cb103"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb103-1"><a href="correlation-and-linear-regression.html#cb103-1" tabindex="-1"></a><span class="fu">glance</span>(m1)</span></code></pre></div>
<pre><code>## # A tibble: 1 Ã 12
##   r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC   BIC
##       &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1     0.947         0.944  15.9      324. 5.88e-13     1  -82.7  171.  174.
## # â¹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;</code></pre>
<p>Then we create a data frame containing all the information required to produce the four plots. Most importantly, this data frame includes the empirical data, the model fitted values, Cookâs distance, residuals. You can follow the ggplot code below to see what variables are used to create each plot. The code provided comes directly from Inchausti Ch.4 with some slight modifications.</p>
<p>The <code>augment</code> function accepts a model object and adds information about each observation in the dataset. These are the values we will plot.</p>
<div class="sourceCode" id="cb105"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb105-1"><a href="correlation-and-linear-regression.html#cb105-1" tabindex="-1"></a><span class="fu">augment</span>(m1)</span></code></pre></div>
<pre><code>## # A tibble: 20 Ã 8
##    shoot  soil .fitted  .resid  .hat .sigma  .cooksd .std.resid
##    &lt;dbl&gt; &lt;int&gt;   &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;
##  1  23.2    60    26.3  -3.12  0.150   16.4 0.00398     -0.212 
##  2  16.2    60    26.3 -10.1   0.15    16.2 0.0419      -0.689 
##  3  52.7    60    26.3  26.4   0.15    14.9 0.285        1.80  
##  4  29.1    60    26.3   2.78  0.15    16.4 0.00316      0.189 
##  5  52.5   120    71.7 -19.2   0.075   15.7 0.0634      -1.25  
##  6  45.7   120    71.7 -26.0   0.075   15.0 0.116       -1.69  
##  7  52.9   120    71.7 -18.8   0.075   15.7 0.0608      -1.22  
...</code></pre>
<p>Notice the table includes one row for each of the observations (only display 7 above). The next step is to store the results of <code>augment(m1)</code> in order to create the plots.</p>
<div class="sourceCode" id="cb107"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb107-1"><a href="correlation-and-linear-regression.html#cb107-1" tabindex="-1"></a>res.m1 <span class="ot">&lt;-</span> <span class="fu">augment</span>(m1)</span></code></pre></div>
<div class="sourceCode" id="cb108"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb108-1"><a href="correlation-and-linear-regression.html#cb108-1" tabindex="-1"></a><span class="co"># ggplots of the residual analysis of of Frequentist simple regression</span></span>
<span id="cb108-2"><a href="correlation-and-linear-regression.html#cb108-2" tabindex="-1"></a>m1.res<span class="ot">=</span><span class="fu">ggplot</span>(<span class="at">data=</span>res.m1, <span class="fu">aes</span>(<span class="at">x=</span>.fitted, <span class="at">y=</span>.resid)) <span class="sc">+</span></span>
<span id="cb108-3"><a href="correlation-and-linear-regression.html#cb108-3" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size=</span><span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb108-4"><a href="correlation-and-linear-regression.html#cb108-4" tabindex="-1"></a>  <span class="fu">theme_bw</span>() <span class="sc">+</span></span>
<span id="cb108-5"><a href="correlation-and-linear-regression.html#cb108-5" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="dv">0</span>) <span class="sc">+</span> </span>
<span id="cb108-6"><a href="correlation-and-linear-regression.html#cb108-6" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x=</span><span class="st">&quot;Fitted values&quot;</span>, <span class="at">y=</span><span class="st">&quot;Standard residuals&quot;</span>) <span class="sc">+</span></span>
<span id="cb108-7"><a href="correlation-and-linear-regression.html#cb108-7" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">axis.title=</span><span class="fu">element_text</span>(<span class="at">size=</span><span class="dv">12</span>), </span>
<span id="cb108-8"><a href="correlation-and-linear-regression.html#cb108-8" tabindex="-1"></a>        <span class="at">axis.text =</span> <span class="fu">element_text</span>(<span class="at">size=</span><span class="dv">10</span>))</span>
<span id="cb108-9"><a href="correlation-and-linear-regression.html#cb108-9" tabindex="-1"></a>m1.res.vs.expl<span class="ot">=</span><span class="fu">ggplot</span>(<span class="at">data=</span>res.m1, <span class="fu">aes</span>(<span class="at">x=</span>soil, <span class="at">y=</span>.resid)) <span class="sc">+</span></span>
<span id="cb108-10"><a href="correlation-and-linear-regression.html#cb108-10" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="at">size=</span><span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb108-11"><a href="correlation-and-linear-regression.html#cb108-11" tabindex="-1"></a>  <span class="fu">theme_bw</span>() <span class="sc">+</span></span>
<span id="cb108-12"><a href="correlation-and-linear-regression.html#cb108-12" tabindex="-1"></a>  <span class="fu">geom_hline</span>(<span class="at">yintercept =</span> <span class="dv">0</span>) <span class="sc">+</span> </span>
<span id="cb108-13"><a href="correlation-and-linear-regression.html#cb108-13" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x=</span><span class="st">&quot;Cd in soil&quot;</span>, <span class="at">y=</span><span class="st">&quot;stnd residuals&quot;</span>) <span class="sc">+</span></span>
<span id="cb108-14"><a href="correlation-and-linear-regression.html#cb108-14" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">axis.title.x=</span><span class="fu">element_text</span>(<span class="at">size=</span><span class="dv">12</span>),</span>
<span id="cb108-15"><a href="correlation-and-linear-regression.html#cb108-15" tabindex="-1"></a>        <span class="at">axis.title.y =</span> <span class="fu">element_blank</span>(),</span>
<span id="cb108-16"><a href="correlation-and-linear-regression.html#cb108-16" tabindex="-1"></a>        <span class="at">axis.text =</span> <span class="fu">element_text</span>(<span class="at">size=</span><span class="dv">10</span>))</span>
<span id="cb108-17"><a href="correlation-and-linear-regression.html#cb108-17" tabindex="-1"></a>m1.Cook<span class="ot">=</span><span class="fu">ggplot</span>(<span class="at">data=</span>res.m1, <span class="fu">aes</span>(<span class="at">x=</span><span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(res.m1),<span class="at">y=</span>.cooksd)) <span class="sc">+</span></span>
<span id="cb108-18"><a href="correlation-and-linear-regression.html#cb108-18" tabindex="-1"></a>  <span class="fu">geom_linerange</span>(<span class="fu">aes</span>(<span class="at">ymin=</span><span class="dv">0</span>, <span class="at">ymax=</span>.cooksd)) <span class="sc">+</span></span>
<span id="cb108-19"><a href="correlation-and-linear-regression.html#cb108-19" tabindex="-1"></a>  <span class="fu">theme_bw</span>() <span class="sc">+</span></span>
<span id="cb108-20"><a href="correlation-and-linear-regression.html#cb108-20" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x=</span><span class="st">&quot;Data point&quot;</span>, <span class="at">y=</span><span class="st">&quot;Cook distance&quot;</span>)<span class="sc">+</span></span>
<span id="cb108-21"><a href="correlation-and-linear-regression.html#cb108-21" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">axis.title=</span><span class="fu">element_text</span>(<span class="at">size=</span><span class="dv">12</span>), <span class="at">axis.text =</span> <span class="fu">element_text</span>(<span class="at">size=</span><span class="dv">10</span>))</span>
<span id="cb108-22"><a href="correlation-and-linear-regression.html#cb108-22" tabindex="-1"></a>m1.qq<span class="ot">=</span><span class="fu">ggplot</span>(res.m1, <span class="at">mapping=</span><span class="fu">aes</span>(<span class="at">sample =</span> .std.resid)) <span class="sc">+</span> </span>
<span id="cb108-23"><a href="correlation-and-linear-regression.html#cb108-23" tabindex="-1"></a>  <span class="fu">stat_qq_point</span>() <span class="sc">+</span></span>
<span id="cb108-24"><a href="correlation-and-linear-regression.html#cb108-24" tabindex="-1"></a>  <span class="fu">theme_bw</span>() <span class="sc">+</span></span>
<span id="cb108-25"><a href="correlation-and-linear-regression.html#cb108-25" tabindex="-1"></a>  <span class="fu">stat_qq_line</span>() <span class="sc">+</span></span>
<span id="cb108-26"><a href="correlation-and-linear-regression.html#cb108-26" tabindex="-1"></a>  <span class="fu">stat_qq_band</span>(<span class="at">alpha=</span><span class="fl">0.3</span>) <span class="sc">+</span></span>
<span id="cb108-27"><a href="correlation-and-linear-regression.html#cb108-27" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&quot;Theoretical quantiles&quot;</span>, <span class="at">y =</span> <span class="st">&quot;Sample quantiles&quot;</span>) <span class="sc">+</span></span>
<span id="cb108-28"><a href="correlation-and-linear-regression.html#cb108-28" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">axis.title=</span><span class="fu">element_text</span>(<span class="at">size=</span><span class="dv">12</span>), </span>
<span id="cb108-29"><a href="correlation-and-linear-regression.html#cb108-29" tabindex="-1"></a>        <span class="at">axis.text =</span> <span class="fu">element_text</span>(<span class="at">size=</span><span class="dv">10</span>))</span>
<span id="cb108-30"><a href="correlation-and-linear-regression.html#cb108-30" tabindex="-1"></a><span class="fu">plot_grid</span>(m1.res,  m1.res.vs.expl, m1.qq, m1.Cook, <span class="at">ncol=</span><span class="dv">2</span>,<span class="at">labels =</span> LETTERS[<span class="dv">1</span><span class="sc">:</span><span class="dv">4</span>],</span>
<span id="cb108-31"><a href="correlation-and-linear-regression.html#cb108-31" tabindex="-1"></a>             <span class="at">align=</span><span class="st">&quot;hv&quot;</span>,<span class="at">label_x=</span><span class="fl">0.85</span>, <span class="at">label_y=</span><span class="fl">0.95</span>) <span class="co"># Fig 4.5 in Inchausti</span></span></code></pre></div>
<p><img src="ASEE_files/figure-html/unnamed-chunk-48-1.png" width="672" /></p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="data-exploration.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
