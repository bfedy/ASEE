[["index.html", "Applied Statistics in Environment and Ecology Chapter 1 Welcome", " Applied Statistics in Environment and Ecology Dr. Brad Fedy 2024-01-06 Chapter 1 Welcome This book is being written to support student progress and learning in ERS 669 Applied Statistics in Environment and Ecology. Notice the present tense in the previous sentence. This book is a work in progress. The book is written using Bookdown which is a package in R that facilitates writing books with R Markdown. The book is provided as .HTML files on LEARN. This allows for a dynamic document allowing you to easily navigate through book sections and copy code. The book files will be updated on LEARN each week in preparation for our in-class programming sessions. …testing revisions "],["introduction-to-r.html", "Chapter 2 Introduction to R 2.1 What is R? 2.2 RStudio 2.3 Before you begin 2.4 Setting a working directory 2.5 Getting help in R 2.6 Defining R objects 2.7 Exporting data frames", " Chapter 2 Introduction to R 2.1 What is R? R is a programming language and software environment for statistical computing and graphics created by Ross Ihaka and Robert GentlemanIhaka R. &amp; Gentleman R. 1996. R: a language for data analysis and graphics. Journal of Computational and Graphical Statistics 5: 299-314. The development and distribution of R is carried out by a group of statisticians known as the R Development Core Team and is distributed under the terms of the GNU Public License. Thus, R can be freely downloaded from the Comprehensive R Archive Network (CRAN) website. The fact that R is free and available across operating systems is one of the main benefits of learning and using R for statistical analysis. There are multiple other freely available programming languages and software what can be used for statistics and data science (e.g., Python), but R is one of the most common for applications in academia and in ecology and environmental studies, specifically. Additionally, there are many commercial statistical packages that are comparable and can be used to similar ends; but, at a cost &gt; $1,000 per license. Installation of R software is straight forward for all major platforms and instructions are provided on the website. Most of the analyses and tasks that are conducted in R are done through the command-line interface and thus, for the non-programmer the learning curve for this software may be steeper than for most point and click statistical programs. There are, however, several advantages of using R and the command-line setup that make putting the effort into learning this program well worth your time: Cost: It’s free! No matter what lab, agency, or company you work with, R can go with you. Simplicity: R is a relatively simple programming language, allowing you to accomplish a lot with minimal lines of code. Diverse and continued package development: In the past, researchers have needed to learn multiple software packages to conduct their analyses and display their results. Because R is open source and relies on user contributed packages, a wide variety of general and specialized analyses and plotting functions can be conducted in R. New packages are developed and existing ones are updated each day. This reduces this need to learn multiple software packages. Integration: R works nicely with other scripting languages and open source software used for a variety of tasks, including Geographic Information Systems (e.g., GRASS, QGIS) and document production (e.g., Markdown - more on this later). Reproducibility: If used properly, command-line scripting keeps detailed records of your analyses. This makes is easy to share, repeat, and modify your work. This is an important point. One of the most important components of good science is replicability. Most research requires many steps from organizing and checking the data to conducting the analysis. Using statistical software that allows you to point and click through your analysis does not usually leave any record of what has been done. Thus, it can be difficult to remember and precisely replicate the data manipulation and analyses. Using command-line scripting requires the analyst to type out a line of code for every action they want to perform. Use of a code hosting platform such as GitHub can further facilitate transparency, reproducibility, version control, and collaboration. We will not cover the use of GitHub explicitly in this course, but it is a potentially useful tool you may encounter as you progress through your research. Self-education: There is a large, active, and (mostly) helpful community of people dedicated to education and training in R programming languages. With enough effort you can find online tutorials or examples for any statistical analysis in R, along with example code. This community is one that you will rely on throughout your R programming and statistical education. This manual will serve multiple purposes. It will be used to provide examples during lectures and code that you will be required to work through on you own. The introduction to R will be brief so that we can get into working with data as quickly as possible. However, there are many excellent resources to facilitate your R-based education. Do not fall behind in your understanding of R. Ask frequent questions of Dr. Fedy and your colleagues. Be sure you understand each line of code. We will use R for all course topics. It is essential that you understand the code in order to understand the data management and statistics that are the focus of the course. This manual is not intended to turn you all into R programming ninjas and statisticians. The intention is to provide you with the tools required for self-education. R is a potentially relevant tool for anyone who uses numbers to understand the world - regardless of your area of interest. When programming in R, there are often multiple ways to solve the same problem. That is part of the fun and challenge of programming. Of course, the different options to completing an analysis increase with increasing complexity of the problem; for simple problems there are usually only a few ways to solve it, for complex problems there are often many ways to solve it. Throughout the course I will demonstrate the code and approaches that have worked for me, my students, and collaborators. However, I intentionally do not provide all the answers and you will need to problem-solve and self-educate in order to successfully complete all the assignments. You will need to install two pieces of software on your computer: R and RStudio. Both of these are free to download. When you open R, you will see the basic command-line console version of R. RStudio is an Integrated Development Environment (IDE) for R, that provides a few more bells and whistles and tends to ease the introduction to R. You do not have to use RStudio, but it is highly recommended. 2.2 RStudio One of the most significant advantages of using the command-line structure in R is that, if done properly, you have detailed documentation for everything that you do. This allows you to easily change, manipulate, and share your code with others. This process is facilitated by writing any code you use into a script file, instead of entering it directly into R. Rstudio can be customized in many ways, but is typically divided into four windows and that provide simultaneous views of important information. The most frequently used windows and tabs are: scripts: Write your code here for a permanent record. console: Essentially R running within RStudio. Displays output and any errors. Environment/Plots: Environment shows you the objects that are currently saved in your workspace and Plots displays any plots that you create. History/Files/Packages: These tabs are less commonly referenced and are fairly self-explanatory. 2.3 Before you begin There are four important things that you should note about R and this manual that will help you as you work through the instructions and exercises: Throughout this book R code is in the grey boxes, with results that you should see in R following this text prefaced by ##. Text following a # are not read by R and are used to comment the code. I use # marks to provide information about a particular command or function. When working in the R console, the up and down arrows can be used to retrieve past commands, saving you from re-typing it. If you see a + prompt, it means R is waiting for more input. Often this means that you have forgotten a closing parenthesis or made some other syntax error. If you cannot figure out its meaning, hit the escape key and start the command fresh. In R, tasks such as calculating means, conducting statistical analysis or generating plots are completed through the use of functions. A function in mathematics is something that relates an input to an output. It has three basic components an input, a relationship, an output. The function defines the relationship. The arguments are the variables or inputs required for the function to produce the desired output. For example, a function to calculate the mean height of everyone in the class would require 1 argument: the height of each person. The function calculates the number of people (n) based on the number of observations and outputs the mean by summing all individual heights and dividing by n. We will go through many examples of functions throughout the course. Functions in R accept arguments, which are used to complete the task (produce output) and use the following syntax: &gt; functionname(argument 1, argument 2, ...) The arguments are always surrounded by (round) parentheses and separated by commas. Some functions (e.g., data()) have no required arguments, but you still need the parentheses. If you type a function name without the parentheses, you will see the code that is the basis for that function including the required arguments. Before you begin any analysis it is important that you tell R the home directory that you will be working in by using the command setwd(“path to folder”). It is important that you include the quotation marks (“”) around the pathname. It is also important that you have write access to the drive that you are using for your working directory. Please keep this in mind if you are using multiple computers. You can set your working directory using the drop down menus in RStudio, by typing out the path manually, or by using the command file.choose(). 2.4 Setting a working directory The first steps for setting a working director typically happen outside of R or RStudio. The first step is to create a new folder on your computer where you want to store all of the work associated with this class or with a particular assignment. Once the folder is created, the second step is to put a file into the folder you created. It does not matter what type of file you put in this folder. Typically, I use a simple blank .txt or the data file (typically .csv) file for examples in class. Once this is complete navigate back to R and type in the following command at the prompt: file.choose() R will then open your computers file management and navigation system. For Mac OS this is Finder, in Windows machines, the Explorer window will open. Within that window, navigate to the working folder you just created and the file you put in it. Then select (“choose”) that file. R will respond by putting the entire path name into your R console window. [1] &quot;/Users/bfedy/Documents/PROJECTS/Rwork/Anything filename.txt&quot; Select everything in the path except for the file name (e.g., the .txt file) and copy and paste it into your setwd command. setwd(&quot;/Users/bfedy/Documents/PROJECTS/Rwork/&quot;) Once this is completed R will put everything it generates and look for any files you pass to it in the “Rwork” file that I created above. Please note: the path names will be different on every computer that you use, so you will need to reset the working director every time you change computers. 2.5 Getting help in R The list of functions available within R is much too great to provide even an adequate summary. There are, however, a wide variety of online resources available that we expect you to draw on throughout this course. Searching “R” and “Type of analysis” in a search engine will often produce code and sometimes tutorials for the analyses you are looking to undertake. To be more specific, you can also include “cran” as a keyword. As you work through this manual and your assignments I encourage you to start a document where you note commands that you find useful with a brief description. This will help save you from learning the same command multiple times. You are learning a new language, please take notes and do not expect yourself to remember everything. You might want to check out some cheat sheets compiled by R as examples. There are also a variety of ways to get help within the R software. If you type a ? followed by the command or help(commandname) will pull up a help file document with the follow structure: Description: Brief description. Usage: For a function, gives the name with all its arguments and the possible options (with the corresponding default values); for an operator gives the typical use. Arguments: For a function, details each of its arguments. Details: Detailed description. Value: If applicable, the type of object returned by the function or the operator. See Also: Other help pages close or similar to the present one. Examples: Some working examples of the function or operator. Other methods for obtaining help when you do not know the exact name of the command you are looking for is to use ?? or help.search with a key word. This will give a list of commands with topics that match your key word. Expect to spend a good amount of time searching for help on the internet while you are programming - particularly at the beginning. It is common to spend as much time (or more) searching the internet as you spend writing code when first starting out. There are many excellent websites and free resources for help and guidance through your R and statistical journey. I cannot list them all here; however, two books that I go back to regularly for R programming help are: R for Data Science R Graphics Cookbook 2.6 Defining R objects A fundamental concept of R is that when R is running, data and results are stored in the active memory in the form of objects, which are all assigned a name by the user. The user can then manipulate these objects through the use of operators or functions. Operators perform tasks including arimetic, logical, and conditional actions. Functions are the work horses of R and are self-contained modules of code that accomplish a specific task. Functions are typically provided data (i.e., values, vectors, dataframes), process that data according to the rules defined in the function, and return a result. Functions can be user-generated, but the majority of functions that you use will be contained within R packages. Much more information will be presented on both these topics throughout the book. Objects can be stored in a variety of ways and it is critical to understand how objects are created, their formats, and what they mean as a first step in learning R. The most basic form of an object is a scalar (i.e., single value), which can be creating using the “assign” &lt;- operator. The assign operator is simply a less than sign followed by a minus sign. The following examples demonstrate the basics. n &lt;- 15 n ## [1] 15 n &lt;- &quot;Applied Statistics&quot; n ## [1] &quot;Applied Statistics&quot; N &lt;- 18 Two things of particular note in the example above, 1) R is case sensitive with n being different than N, and 2) if an object exists and is assigned a new name, the previous value is erased without any warning. Thus, it is important to pay attention to the variables you create and those which are already stored in your active memory. You can get a list of the names of variables in memory with the ls() command or for the variables with some details on the objects use ls.str(). In RStudio, you can also see all the objects you have created in the ‘Environment’ tab. To remove objects from the active memory you can use the rm('object') command. Look at the example below and note that we use rm(list=ls()) to list and remove all objects in the active memory. It is good practice to start all of your new scripts with the rm(list=ls() command. That way you know the script is not dependent on any objects previously stored in your Environment (i.e., working memory). When you run the code below, you will see the corresponding values show up in R console. If you want to save those values for later use, you will need to assign them an object name. Y &lt;- c(2,2,3,5,6,7,10,11,11,12) # create a vector of these numbers Y[4] # select the fourth value ## [1] 5 Y[1:5] # select the first five values ## [1] 2 2 3 5 6 Y[5:10] # select the last five values ## [1] 6 7 10 11 11 12 To select or identify values within two dimensional data such as matrices and data frames you will need two numbers (separated by a comma) in the square brackets. The first number specifies the row or range of rows, and the second number specifies the column or range of columns (Dr. Fedy finds it helpful to think of RC Cola to remember the order: rows, then columns. You may not have heard of RC Cola, but you get the point, think of something meaningful to you). For data frames you may also want to extract the column headers of names of columns, which you can replace if you wish to change the name of one or more columns. Below are some examples executed using base R. There are multiple packages that make the selection of data and values more intuitive (much more on those later). # Set up a 2 dimensional data frame options(digits = 3) # set significant digits in R snake.df &lt;- data.frame(species = rep(&quot;P.gloydi&quot;, 20), sex = factor(rep(c(&quot;male&quot;, &quot;female&quot;),10)), mass = runif(n=20, min=0.5, max=1.5), length = runif(n=20, min=50, max=100)) snake.df ## species sex mass length ## 1 P.gloydi male 0.843 88.2 ## 2 P.gloydi female 0.762 53.1 ## 3 P.gloydi male 0.738 54.3 ## 4 P.gloydi female 1.162 57.3 ## 5 P.gloydi male 0.987 99.2 ## 6 P.gloydi female 1.214 71.4 ## 7 P.gloydi male 0.838 74.9 ## 8 P.gloydi female 0.840 87.6 ## 9 P.gloydi male 0.629 68.1 ## 10 P.gloydi female 0.584 67.7 ## 11 P.gloydi male 0.975 93.0 ## 12 P.gloydi female 0.754 98.0 ## 13 P.gloydi male 1.329 98.7 ## 14 P.gloydi female 0.833 71.1 ## 15 P.gloydi male 1.180 86.0 ## 16 P.gloydi female 0.530 81.4 ## 17 P.gloydi male 0.793 97.8 ## 18 P.gloydi female 1.382 75.8 ## 19 P.gloydi male 0.684 52.2 ## 20 P.gloydi female 1.416 83.8 Be sure that you understand what is happening in each of the lines of code above. This would be a good time to use the help functions in R to help you understand. # The output from the following code will show up in your R console names(snake.df) # extract names of columns ## [1] &quot;species&quot; &quot;sex&quot; &quot;mass&quot; &quot;length&quot; snake.df[ , 1] # extract the first column (note: the space are not necessary) ## [1] &quot;P.gloydi&quot; &quot;P.gloydi&quot; &quot;P.gloydi&quot; &quot;P.gloydi&quot; &quot;P.gloydi&quot; &quot;P.gloydi&quot; ## [7] &quot;P.gloydi&quot; &quot;P.gloydi&quot; &quot;P.gloydi&quot; &quot;P.gloydi&quot; &quot;P.gloydi&quot; &quot;P.gloydi&quot; ## [13] &quot;P.gloydi&quot; &quot;P.gloydi&quot; &quot;P.gloydi&quot; &quot;P.gloydi&quot; &quot;P.gloydi&quot; &quot;P.gloydi&quot; ## [19] &quot;P.gloydi&quot; &quot;P.gloydi&quot; snake.df[1, ] # extract the first row ## species sex mass length ## 1 P.gloydi male 0.843 88.2 snake.df[ , 1:3] # extract the first 3 columns ## species sex mass ## 1 P.gloydi male 0.843 ## 2 P.gloydi female 0.762 ## 3 P.gloydi male 0.738 ## 4 P.gloydi female 1.162 ## 5 P.gloydi male 0.987 ## 6 P.gloydi female 1.214 ## 7 P.gloydi male 0.838 ## 8 P.gloydi female 0.840 ## 9 P.gloydi male 0.629 ## 10 P.gloydi female 0.584 ## 11 P.gloydi male 0.975 ## 12 P.gloydi female 0.754 ## 13 P.gloydi male 1.329 ## 14 P.gloydi female 0.833 ## 15 P.gloydi male 1.180 ## 16 P.gloydi female 0.530 ## 17 P.gloydi male 0.793 ## 18 P.gloydi female 1.382 ## 19 P.gloydi male 0.684 ## 20 P.gloydi female 1.416 snake.df[1:4,1:3] # extract the first 3 columns and 4 rows ## species sex mass ## 1 P.gloydi male 0.843 ## 2 P.gloydi female 0.762 ## 3 P.gloydi male 0.738 ## 4 P.gloydi female 1.162 It is also possible using base R to call individual columns of a data frame using the names of the data frame and the column separated by a $. snake.df$sex ## [1] male female male female male female male female male female ## [11] male female male female male female male female male female ## Levels: female male snake.df$length ## [1] 88.2 53.1 54.3 57.3 99.2 71.4 74.9 87.6 68.1 67.7 93.0 98.0 98.7 71.1 86.0 ## [16] 81.4 97.8 75.8 52.2 83.8 2.7 Exporting data frames As we alluded to earlier, in reality R’s main function is for analyzing and graphing data, and different programs such as excel or other database programs will be used when collecting and entering data. Thus, being able to import and export data from R is essential. Here, we will first introduce you to the write.table() function, which is the primary function used to export a data fame. The main parameters that you need to be aware of are listed below, but you can use R help ?write.table to get a list of all options and more information. One of the most important arguments in this function is the sep argument, which specifies how each column in the data frame will be separated. As a default, I tend to prefer the sep=\",\" option which allows you to open your data a comma separated file (.csv) which is possible in most editors that you will want to use. If you do not provide a file name (as we do below) for the argument file then the output is printed to the screen. After you create the table below try opening it in a text editor or Excel (or some other spreadsheet software). Be sure you have set your working director setwd(\"filename\") so you know where the output will be stored on your machine. write.table(snake.df, sep=&quot;,&quot;, row.names=FALSE, col.names=TRUE) 2.7.1 Installing and loading packages An important component and one of the main advantages of R is that users can write and provide packages. These packages are essentially a series of functions that have been compiled together and can be loaded into R to make these functions available. Without installing and loading these packages these functions are not available. Loading packages in R can be done in R from the command-line if you know the package that you wish to install. Two packages that we will need in later models are the lattice and grid packages. These can be installed using the input below, with dependencies=TRUE specifying that you wish all packages that are needed to be installed at the same time. You may get a pop up window asking if you want to install these in a local directory. If so you should answer yes. You will likely also be asked to specify a CRAN mirror (i.e., download location), so just choose Canada (ON). install.packages(&quot;lattice&quot;, dependencies = TRUE) install.packages(&quot;gridBase&quot;, dependencies = TRUE) In addition to installing packages through the command line you can search for an install packages using the ‘Packages – Install Packages’ within RStudio. Once packages are installed, they must also be loaded locally before you can use the functions contained within the packages. You load packages with the library(\"packagename\") command (replace packagename with the name of the package you are loading). "],["operators.html", "Chapter 3 Operators 3.1 Manipulating R objects", " Chapter 3 Operators 3.1 Manipulating R objects In the previous chapter, I summarized the main types of R objects and how these can be stored in the active memory. Most analyses and tasks that you will complete in R will require you to manipulate these objects and this can be done through a variety of methods. Most of these, however, will require the use of R operators, which are used to manipulate or subset all types of R objects. There are three basic categories of operators that you should be aware of and are summarized below. ARITHMATIC Operator Description + addition - subtraction * multiplication / division COMPARISON Operator Description &gt; greater than &lt; less than &gt;= greater than or equal to &lt;= less than or equal to == equal to != not equal to LOGICAL Operator Description X &amp; Y and (X and Y) X|Y or (X or Y) ! X not (not X) The simplest R operators are basic arithmetic, which basically allows R to act as a calculator. A couple things to note in the examples below, 1) The results of an arithmetic expression can be stored in memory by assigning the result to an object (for simplicity we do not do this for many of the examples) and, 2) when applying operators to vectors or matrices the operation will be conducted on each element in the vector (not the case for data frames where you must specific a column). 2 + 2 # addition ## [1] 4 45*90 # multiplication ## [1] 4050 sqrt(81) # calculate the square root ## [1] 9 x &lt;- 2 + 2; y &lt;- 45*90; z &lt;- sqrt(81) x; y; z ## [1] 4 ## [1] 4050 ## [1] 9 x &lt;- c(20:30) x ## [1] 20 21 22 23 24 25 26 27 28 29 30 x * 10 ## [1] 200 210 220 230 240 250 260 270 280 290 300 The output in each of the above examples above begins ## [1]. The ## was explained in the previous Chapter and exists at the beginning of output in a bookdown document so that it will not run if you copy and paste it into R. The [1] is used to index a value in a vector. For example in the vector x that we created above the first value is 20 and is indexed by [1] if we want the 3rd value in the vector we can retrieve it by using the indexing as follows: x[3] ## [1] 22 Vector arithmetic is an important concept and will be essential when working with real data and you want to manipulate specific columns using an operator. Below we run through a series of examples where this is the case. Be sure to check the objects and output after each line of code. If you do not understand why you got the output you did please ask. It is essential that you understand these actions. They are the foundation. If you have only a vague understanding of what is happening at this point, it will make all subsequent assignments much more difficult. Simply copying from this manual without examining the output will teach you little about R. Typically, we enter our data into a .csv file and import that data into R (more on this later). For example purposes, we will create a basic simulated dataset using code. Consider the following data frame of the widths and lengths of snakes: options(digits = 3) # set significant digits in R snake.df &lt;- data.frame(species = rep(&quot;P.gloydi&quot;, 20), sex = factor(rep(c(&quot;male&quot;, &quot;female&quot;),10)), mass = runif(n=20, min=0.5, max=1.5), length = runif(n=20, min=50, max=100)) snake.df ## species sex mass length ## 1 P.gloydi male 1.033 82.8 ## 2 P.gloydi female 1.188 97.4 ## 3 P.gloydi male 1.333 62.9 ## 4 P.gloydi female 0.830 93.5 ## 5 P.gloydi male 0.848 88.1 ## 6 P.gloydi female 1.378 54.5 ## 7 P.gloydi male 1.100 95.9 ## 8 P.gloydi female 1.082 62.3 ## 9 P.gloydi male 0.566 85.1 ## 10 P.gloydi female 1.408 81.6 ## 11 P.gloydi male 1.337 72.4 ## 12 P.gloydi female 0.731 84.0 ## 13 P.gloydi male 0.867 57.4 ## 14 P.gloydi female 1.315 88.7 ## 15 P.gloydi male 1.004 84.1 ## 16 P.gloydi female 0.510 80.0 ## 17 P.gloydi male 1.453 90.8 ## 18 P.gloydi female 1.167 65.9 ## 19 P.gloydi male 0.940 52.7 ## 20 P.gloydi female 0.768 56.0 Suppose you want to convert the weight of the snakes from kilograms to grams (i.e., multiply by 1000) or create a new variable body condition, which is the weight divided by the length. Both of these can be easily done using an arithmetic operator. Recall, that if you want to call a specific column you simply use the data frame name and the column name separated by a $. If this column exists it will be replaced and if it is does not it will be created. # Replace mass column with mass in kg snake.df$mass &lt;- snake.df$mass*1000 snake.df$body.cond &lt;- snake.df$mass/snake.df$length snake.df ## species sex mass length body.cond ## 1 P.gloydi male 1033 82.8 12.48 ## 2 P.gloydi female 1188 97.4 12.20 ## 3 P.gloydi male 1333 62.9 21.18 ## 4 P.gloydi female 830 93.5 8.88 ## 5 P.gloydi male 848 88.1 9.62 ## 6 P.gloydi female 1378 54.5 25.28 ## 7 P.gloydi male 1100 95.9 11.47 ## 8 P.gloydi female 1082 62.3 17.38 ## 9 P.gloydi male 566 85.1 6.65 ## 10 P.gloydi female 1408 81.6 17.25 ## 11 P.gloydi male 1337 72.4 18.48 ## 12 P.gloydi female 731 84.0 8.70 ## 13 P.gloydi male 867 57.4 15.12 ## 14 P.gloydi female 1315 88.7 14.83 ## 15 P.gloydi male 1004 84.1 11.94 ## 16 P.gloydi female 510 80.0 6.37 ## 17 P.gloydi male 1453 90.8 15.99 ## 18 P.gloydi female 1167 65.9 17.72 ## 19 P.gloydi male 940 52.7 17.83 ## 20 P.gloydi female 768 56.0 13.70 A second important class of operators are the comparison operators, which are often used to subset your data in meaningful ways. For example, consider a situation that requires you subset the snake data by sex and you want only female snakes. Here you would use the == operator, combined with the square brackets [ ]. The square brackets were introduced above for indexing a vector. They can also be used to index matrices and dataframes (i.e., 2-dimensional objects…and beyond). which we know from earlier can be used to subset your data. In this case however, instead of using numbers to select columns, we will use an operator to select rows that are equal to “male”. You can also use other comparison operators such as greater than ‘&gt;’ or less than ‘&lt;’. Finally, we combine a comparison with a logical operator. Remember that when subsetting with the [ ] the first value indicates rows and the value after the “,” indicates columns. When you want all columns, leave the number after the “,” blank, and leave the number before the “,” blank when you want all rows. "],["data.html", "Chapter 4 Data 4.1 Writing scripts 4.2 Importing, viewing, and editing data", " Chapter 4 Data 4.1 Writing scripts When writing R scripts there are a couple things to keep in mind that will make your life much easier. Future you, will thank present you. Most of the time you will write your scripts in an R script file and then send your commands to the R Console from that file. Always start your code with a text header that provides information about your script file so that future you (and others) know what the script does and can reproduce your analysis. At a minimum I suggest the following: Brief description of the script (i.e., a title) The date the script was created or last modified (there are more elegant ways to keep track of this that we will address later) The author of the script R version used when creating the script (R packages change. This is important.) Other items that I ask everyone in my research group to include are: Packages required to run the code (there are more elegant ways to keep track of this that we will also address alter) Data inputs and sources Data outputs Here is an example of the Header text I try to include in all my scripts. Obviously, not all this information is available at the beginning of an analysis, but I encourage you to update your header as you go. #=============================================================================# # HEADER --------------------------------------------------------------------- # # Author: Brad Fedy # # R.Version(): Version 4.3.1 # # Project: # Grassland birds habitat selection # # Required libraries: # Mumin, data.table # # Required input files: # 1. Bird location data e.g., &quot;species.routes.SPPI.csv&quot; # 2. Covariate values at each scale creaeted in step _3.. Example naming: # &quot;SPPI.covariates.`scale&#39;.csv&quot; # # Steps: # 1. PREPARE DATA FOR MODELLING # 2. RUN UNIVARIATE MODELS # 3. CORRELATIONS AMONG TOP COVARIATES # 4. UNIVARIATE MODELS FOR ALL CORRELATED PAIRS # # Output files: # 1. Table of univariate results across all scales for each variable. # e.g., &quot;SPPI.model.result.&quot;,env[[s]],&quot;.csv&quot; # 2. Table of univariate results of each highly correlated pair # e.g., &quot;SPPI.cor.var.models.&quot;,x,&quot;.csv&quot; where `x&#39; is a number 1 to n pairs # # Date: 2023/05/13 # # NOTES: #=============================================================================# rm(list=ls()) setwd(&quot;/Users/bfedy/grassland.birds/DATA/Bird.Data&quot;) library(&quot;MuMIn&quot;) library(&quot;data.table&quot;) The formatting of your scripts can be a matter of personal preference. There are a number of style guides available. Hadley Wickham has one associated with his Advanced R book and Google’s R Style Guide can be found in multiple locations on the internet. Basically, try to make it readable, easily understood, and do not be shy with your use of # comments throughout. You will be grateful later. Really…. more comments are almost always better. 4.2 Importing, viewing, and editing data Greater Sage-Grouse (Centrocercus urophasianus; hearafter sage-grouse) are ground nesting bird, distributed across the intermountain west with a range that across 11 states and 2 Canadian provinces. They are listed under the Canadian Species At Risk Act and of conservation concern throughout their distribution in the United States. There distribution is roughly consonant with the distribution of sagebrush (Artemisia sp.) habitat. The major negative impacts to sage-grouse are habitat loss, invasice species, and energy development across their range. Sage-grouse have a lek mating system in which males congregate at breeding sites (i.e., leks) each spring to compete for mating opportunities using elaborate ‘strutting’ displays to attract females. There have been large efforts across their range to collect detailed ecological and population data - typically focused on lek sites. Approximately 35% of the remaining sage-grouse occur in Wyoming. Long term monitoring data exists for many of the active leks across Wyoming are which are visited 1-3 times per lek season by biologists who count the number of males displaying at a lek. By convention, the highest number of males recorded across all visits is used as the lek size for a given year. These counts are considered a good index of overall population size and are used to assess trends in population numbers. This ongoing effort was started in 1948 and has resulted in a large dataset with thousands of leks where males have been counted and recorded each year. We will use some of these data to demonstrate approaches that can be used visualize and analyze a real ecological dataset. Given issues with data ownership and distribution, the data you will be working with are based on real data, but I have adjusted the data by introducing random variation into the counts and spatial lek locations. So the actual data you are working with have similar distributions and challenges associated with the real data, but but are not accurate in terms of lek locations and lek counts. Th To get you started, we have provided a comma separated values file, which includes summary information on 718 leks from the southern half of the Wyoming. In the figure below you can see distribution of these leks in relation to their range (light grey) across this region. We have summarized the time-series of the leks described and mapped above, by calculating a variety of statistics on the time series form 1982-2013 (“140123wysg_leks.csv”). Before beginning, get associated with this data by opening it in a text editor and excel. A description of data comprised for each column is listed below. Name Description lekid name of lek x UTM easting coordinates y UTM northing coordibnates area name of the management ares (i.e., unit) per_miss percent of years where lek counts were not conducted per_zero percentage of years where no individual males were counted at a lek site mean_count mean number of peak males counted across the time series sd_count standard deviation of peak males counted across the time series min_count minimum number of peak males counted across the time series max_count maximum number of peak males counted across the time series len_road length (m) of raods within 5 km radio of lek mean_ppt mean of July, August, and September 1981-2010 total precipitation for climate zone surrounding lek mean_sage mean sagebrush percent cover within 5 km radius of lek mean_herb mean herbaceous percent cover within 5 km radius of lek mean_nest mean nesting habitat suitability for sage-grouse (range: low(0) to high(1)) within 5km radius of lek Once you are familiar with the data table, open Rstudio and setup a script file (do not forget to save the script file) that you will use to enter your commands throughout all exercises. You can translate your commands from the script to the concole (i.e., run your commands), by highlighting the commands and hitting control-enter on the keyboard. Make sure you input header information before you begin entering commands! It is also important to set your working directory to the directory where your data is saved before attempting to import the data. # set your working directory leks &lt;- read.table(&quot;DATA/140123wysg_leks.csv&quot;, sep=&quot;,&quot;, header=TRUE,quote=&quot;&quot;,comment.char=&quot;&quot;, stringsAsFactors=FALSE) # read in the data head(leks) # look at the first few observations ## lekid x y area per_miss per_zero mean_count sd_count ## 1 D-Alkali Draw -1117044 2253178 D 46.9 50.0 34.529 17.6 ## 2 D-Antelope Draw -1154827 2281868 D 28.1 40.6 52.391 45.4 ## 3 D-Bench Corral -1148670 2278415 D 25.0 43.8 33.583 28.6 ## 4 D-Big John -1099343 2250458 D 50.0 50.0 87.875 19.1 ## 5 D-Big Sandy Flat -1088257 2252134 D 25.0 90.6 0.875 3.0 ## 6 D-Billie&#39;s Draw -1152847 2275456 D 50.0 71.9 20.062 24.6 ## min_count max_count len_road mean_ppt mean_sage mean_herb mean_nest ## 1 0 67 172473 24.8 14.8 15.9 0.484 ## 2 0 133 188797 28.2 16.5 16.7 0.525 ## 3 0 87 219941 25.0 14.7 13.8 0.463 ## 4 56 119 134197 24.9 14.5 14.3 0.434 ## 5 0 14 185886 25.6 16.1 17.0 0.487 ## 6 0 73 171083 26.5 16.0 13.4 0.555 There are often issues when importing data. Typically these are caused by variables that contain character data. You can visually inspect your imported data using the view() function or by clicking the data table in your Environment in RStudio. It is useful to always use summary functions to ensure that everything imported as expected. Here we use two such functions. dim(leks) # provide dimensions of the data frame ## [1] 718 15 summary(leks) ## lekid x y area ## Length:718 Min. :-1236354 Min. :2043637 Length:718 ## Class :character 1st Qu.:-1129942 1st Qu.:2122548 Class :character ## Mode :character Median : -980625 Median :2166752 Mode :character ## Mean :-1012654 Mean :2169603 ## 3rd Qu.: -910777 3rd Qu.:2208761 ## Max. : -790076 Max. :2319509 ## per_miss per_zero mean_count sd_count min_count ## Min. : 0.0 Min. : 0.0 Min. : 0.0 Min. : 0.0 Min. : 0.0 ## 1st Qu.: 0.0 1st Qu.: 46.9 1st Qu.: 2.6 1st Qu.: 5.8 1st Qu.: 0.0 ## Median :12.5 Median : 65.6 Median : 10.3 Median : 11.7 Median : 0.0 ## Mean :19.9 Mean : 62.2 Mean : 16.6 Mean : 15.6 Mean : 1.2 ## 3rd Qu.:34.4 3rd Qu.: 81.2 3rd Qu.: 23.8 3rd Qu.: 20.8 3rd Qu.: 0.0 ## Max. :81.2 Max. :100.0 Max. :159.4 Max. :104.5 Max. :80.0 ## max_count len_road mean_ppt mean_sage mean_herb ## Min. : 0 Min. : 35552 Min. :15.8 Min. : 0.00 Min. : 3.8 ## 1st Qu.: 22 1st Qu.:122144 1st Qu.:20.9 1st Qu.: 8.00 1st Qu.: 8.2 ## Median : 42 Median :155570 Median :23.9 Median : 9.99 Median :11.5 ## Mean : 56 Mean :160325 Mean :24.4 Mean :10.19 Mean :12.8 ## 3rd Qu.: 77 3rd Qu.:193281 3rd Qu.:27.4 3rd Qu.:12.27 3rd Qu.:16.3 ## Max. :350 Max. :396330 Max. :35.8 Max. :19.76 Max. :51.2 ## mean_nest ## Min. :0.000 ## 1st Qu.:0.101 ## Median :0.162 ## Mean :0.198 ## 3rd Qu.:0.262 ## Max. :0.695 It is useful to check the format of your data. We previously discussed some of the potential issues of storing columns as factors. Check to see if any columns are stored as factors. If needed re-import the data so that no columns are factors. 4.2.1 Summarizing and manipulating data One of the key components to any analysis is the ability to summarize and manipulate your data. There are almost as many ways to accomplish common data summary tasks as there are users. We provided some of these basic functions earlier. However, in many cases you will want to summarize a data set using a grouping variable. For example, suppose we wish to determine the mean number of males counted within each of the management areas. One way this can be done using base R is with the tapply() function which applies a function to each group within a vector. To use tapply you supply i) a numeric vector of a data frame to be summarized (X; i.e., your variable of interest), ii) a grouping factor (INDEX), and iii) a function (FUN) to summarize the numeric vector: tapply(X=leks$mean_count,INDEX=leks$area,FUN=mean) ## D E F G H ## 30.96 28.49 8.53 16.68 13.14 This example used mean; however, tapply can be used with any built in function (e.g., sum(), sd(), length(), max(), min()) or with a customized function of your own design. In addition to this flexibility, more than one grouping factor can be incorporated. For example, suppose we had some idea of a breakpoint between high and low quality nesting habitat for leks and we wanted to calculate a summary table describing the mean male counts for low and high suitability leks within each management area. To accomplish this we first need to create an additional variable, which groups the leks into ‘low’ or ‘high’ suitability groupings. leks$nest_type&lt;-&quot;low&quot; # create a nesting suitablity variable leks[leks$mean_nest&gt;0.3,&quot;nest_type&quot;]&lt;-&quot;high&quot; # change rows with values &gt; 0.3 to &#39;high&#39; leks_sort&lt;-leks[order(leks$nest_type),] # sort data frame by nest_type Above we also use the order() function to sort the data frame by our new variable. Note that when we sort the data frame we create a new data frame, so the original data remains unmodified. View the sorted data frame to ensure the new variable is correctly assigned. After creating this variable you can use the tapply function with both the management area (area) and the newly created nest_type variable: lek_summary&lt;-aggregate(x=leks_sort$mean_count,by=list(leks_sort$area,leks_sort$nest_type), FUN=mean) lek_summary ## Group.1 Group.2 x ## 1 D high 35.78 ## 2 E high 33.26 ## 3 F high 17.14 ## 4 G high 30.51 ## 5 H high 8.43 ## 6 D low 21.84 ## 7 E low 23.56 ## 8 F low 8.02 ## 9 G low 14.63 ## 10 H low 13.60 The original data came in the form of a time series over multiple years. These summaries were created using the apply() function, which can be used to apply a function across rows (MARGIN=1) or down columns (MARGIN=2). To use these summaries, load the time series data using the readtable function. lek_ts&lt;-read.table(&quot;DATA/wysg_peak_males_ts_sub.csv&quot;,sep=&quot;,&quot;,header=TRUE) head(lek_ts) ## lekid X2008 X2009 X2010 X2011 X2012 X2013 ## 1 B-Blue Mesa 24 16 13 16 13 9 5 ## 2 B-Grass Creek 2 0 17 4 5 12 7 ## 3 B-Hoodoo 3 31 12 11 9 5 2 ## 4 B-Logging Road 24 42 31 10 12 12 ## 5 C-Innes 34 29 19 14 15 12 ## 6 C-Upton-Fairview 11 13 12 10 17 9 # take mean across last 5 rows and add to new column lek_ts$mean_count&lt;-apply(X=lek_ts[,2:7],MARGIN=1,FUN=mean) head(lek_ts) ## lekid X2008 X2009 X2010 X2011 X2012 X2013 mean_count ## 1 B-Blue Mesa 24 16 13 16 13 9 5 12.0 ## 2 B-Grass Creek 2 0 17 4 5 12 7 7.5 ## 3 B-Hoodoo 3 31 12 11 9 5 2 11.7 ## 4 B-Logging Road 24 42 31 10 12 12 21.8 ## 5 C-Innes 34 29 19 14 15 12 20.5 ## 6 C-Upton-Fairview 11 13 12 10 17 9 12.0 "],["data-visualization.html", "Chapter 5 Data Visualization 5.1 Plotting data 5.2 Manipulating plots 5.3 Adding low-level plotting functions 5.4 Facet plots", " Chapter 5 Data Visualization 5.1 Plotting data In addition to statistical analysis and programming capabilities, R produces publishable quality graphics. Base R has the capacity to create excellent graphs; however, the syntax can sometimes feel a little clunky. Most recent graphing examples that you find online will make use of the wildly popular and exceptionally user-friendly package ggplot2. We will use this package throughout this book. There is a excellent book that describes all the details of this package called the R Graphics Cookbook. This chapter will introduce some basic level plotting and options and show you how to modify these options to customize plots. The next chapter will cover Data Exploration (i.e., summarizing and visualizing your data in order to inform your data analysis approach). The first step is to install and load and the ggplot2 package. We will also load a data set containing information on Eastern Fox Snakes (“efs_growth.csv”). This data set investigates eastern fox snake (Pantherophis gloydi) growth rates (“grate”) in two different regions in Ontario (“region”). Reptiles have indeterminate growth (i.e., grow throughout their entire lives), but their growth rates decline as they get older. The dataset also includes measurements on snout-to-vent length (“svl”; a proxy of age). install.packages(&quot;ggplot2&quot;) library(&quot;ggplot2&quot;) efs&lt;-read.csv(&quot;DATA/efs_growth.csv&quot;,sep=&quot;,&quot;,header=TRUE) head(efs) ## Year days ID region sex SVL grate ## 1 2007 333 4415685726 Essex f 1270 -0.353 ## 2 2010 369 4940181800 Essex m 858 1.178 ## 3 2009 730 4962583337 Essex m 1042 0.340 ## 4 2007 357 133637351a Essex f 1070 0.643 ## 5 2007 415 133768254a Essex f 915 0.815 ## 6 2008 711 134713166a Essex m 816 0.853 str(efs) ## &#39;data.frame&#39;: 207 obs. of 7 variables: ## $ Year : int 2007 2010 2009 2007 2007 2008 2008 2007 2009 2008 ... ## $ days : int 333 369 730 357 415 711 378 368 371 354 ... ## $ ID : chr &quot;4415685726&quot; &quot;4940181800&quot; &quot;4962583337&quot; &quot;133637351a&quot; ... ## $ region: chr &quot;Essex&quot; &quot;Essex&quot; &quot;Essex&quot; &quot;Essex&quot; ... ## $ sex : chr &quot;f&quot; &quot;m&quot; &quot;m&quot; &quot;f&quot; ... ## $ SVL : num 1270 858 1042 1070 915 ... ## $ grate : num -0.353 1.178 0.34 0.643 0.815 ... Imagine that we wanted to make a plot that displayed growth rate as a function of their snout-to-vent length (“svl”; a proxy of age). To plot with ggplot, we use the function ggplot(), which creates a coordinate system that we can then add layers to. The first argument for the ggplot() function is the data set that we want to use. In our case, the data set we want to use is ‘efs’, which we loaded into our environment above. We then add one or more layers to our ggplot with a geom_ function. Geoms are geometric objects, like points, lines, or bars that we can add to our ggplot coordinate system. In this example, we will create a scatterplot with geom_point comparing “svl” to “grate”. ggplot(data=efs,aes(x=SVL, y=grate)) + geom_point() As expected, we can see the negative relationship between snout-to-vent length (i.e, age) and growth. This helps us visualize the data, but we can improve the plot by adding better labels and making other manipulations to better visualize our data. 5.2 Manipulating plots We can specify a number of parameters within the geom_ function. For example, we can modify the colour, transparency, shape, or size of the points. Don’t forget that you can also use ?geom_point to understand what else you can do with this code or other geom_ functions. ggplot(data=efs, aes(x= SVL, y=grate)) + geom_point(colour = &quot;blue&quot;, # change the colour of points alpha = 0.50, # change the transparency shape = 20, # change the shape of the point size = 2) # change the size of the point R use numeric codes to represent different plot characters (pch), specified by the shape option in ggplot2. You can find indexes to these characters online or by typing ?pch into the console. R defaults to the variable name. Frequently, these are difficult to decipher outside of your research group and need to be revised for publication and dissemination. ggplot(data=efs, aes(x= SVL, y=grate)) + geom_point()+ labs(x = &quot;Snout-to-Vent Length (mm)&quot;, y = &quot;Growth Rate (mm/day)&quot;) + scale_x_continuous(breaks = seq(0,1500,100))+ scale_y_continuous(breaks = seq(-1,3,0.5)) All aspects of the graph are adjustable. It is often helpful to sketch your ideal plot first by hand and then figure out how plot it using ggplot2. You can also use themes in ggplot2 to change the overall appearance of your plot and control all non-data display. There are many built-in themes and you can review their details online. You can also create your own theme() that you can replicate across analyses so that all your plots have a similar format. Themes are covered in Chapter 9 of the R Graphics Cookbook. A simple example of how to add a theme to your plot and a legend is here: ggplot(data=efs, aes(x= SVL, y=grate, colour=region)) + geom_point() + labs(x = &quot;Snout-to-Vent Length (mm)&quot;, # change x label y = &quot;Growth Rate (mm/day)&quot;, # change y label colour = &quot;Region&quot;) + # colour code points by Region scale_x_continuous(breaks = seq(0,1500,100)) + # specify x-scale scale_y_continuous(breaks = seq(-1,3,0.5)) + # specify y-scale theme_classic() # specify theme 5.3 Adding low-level plotting functions Low-level plotting functions plot on already existing plots. For example suppose we had preformed a regression and wanted to add a regression line to our plot to further visualize the negative relationship between age and growth rate of Eastern fox snakes. We can do this using the geom_smooth function to add the regression line to the plot. ggplot(data=efs, aes(x= SVL, y=grate, colour=region)) + geom_point()+ labs(x = &quot;Snout-to-Vent Length (mm)&quot;, y = &quot;Growth Rate (mm/day)&quot;, colour = &quot;Region&quot;) + scale_x_continuous(breaks = seq(0,1500,100))+ scale_y_continuous(breaks = seq(-1,3,0.5)) + theme_classic()+ # add a theme geom_smooth(method=&quot;lm&quot;) # add a regression line to our plot ## `geom_smooth()` using formula = &#39;y ~ x&#39; 5.4 Facet plots When displaying data it is often useful to have multiple items highlighted on a single plot or to have a multi-panel plot for related data. Below we provide an example using this data to make a facet plot with the facet_grid function. Again make sure you use help to understand what is happening with each line of code if it is not familiar to you (e.g, ?facet_grid). Here, we are going to create a plot that compares growth rate with age for females and males at each different region. We will produce two separate plots for each region that compare males and females. We will also add regression lines. ggplot(data=efs, aes(x= SVL, y=grate, colour=sex)) + geom_point()+ labs(x = &quot;Snout-to-Vent Length (mm)&quot;, y = &quot;Growth Rate (mm/day)&quot;, colour = &quot;Region&quot;) + scale_x_continuous(breaks = seq(0,1500,100)) + scale_y_continuous(breaks = seq(-1,3,0.5)) + theme_classic() + # add a theme geom_smooth(method=&quot;lm&quot;) + # add a regression line to our plot facet_grid(vars(region)) # add the facet grid for a variable of your choice ## `geom_smooth()` using formula = &#39;y ~ x&#39; Developing your plots in R is a great way to improve and maintain your R skills. This may seem like extra work compared to point-and-clicking your way through Excel to create a similar plot; but coding your plots allows for easy replication, update, sharing, and transparency. The code may look fairly complex to create a simple looking plot, but much of the code repeats and can be reused in future efforts. The last thing we will do is save our plot to our working directory. You can do this through RStudio with the Export function in the plotting window; however, it is more useful to learn how to save with code so that you can make specifications to your figure size and file type. This can be accomplished using the ggsave function to output the figure with a specific size and file type. All journals have specifications regarding the size, file type, and resolution of figures for publication. The ggsave function an associated options ensures that you meet those criteria when submitting your research for publication. ggsave (&quot;efs.figure.pdf&quot;, device = &quot;pdf&quot;, width = 7.5, height = 5, units = &quot;in&quot;) "],["data-exploration.html", "Chapter 6 Data Exploration 6.1 Outliers 6.2 Homoscedasticity (homegeneity of variance) 6.3 Distribution of the response variable 6.4 Collinearity among x variables", " Chapter 6 Data Exploration Before conducting any statistical analysis you need to graphically explore your data. There are many packages within R to assist you with the process of data exploration. Many of the reasons behind data exploration and methodology are outlined in a review article in Methods in Ecology and Evolution by Zuur et al. (2010) . We are pattern matching machines. We have evolved to visually detect patterns among the chaos. It is important to use that strength to visually inspect your data, understand its nuances, and allow that information to help inform your analyses in model structure. We will cover general (and generlized) linear models in detail later in the course. However, it is necessary to introduce a simple model here to better explain why we recommend generating the plots in this chapter. \\[y_i = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ...+ \\beta_jx_i + \\epsilon_i\\] In this model, we are trying to understand the relationship between a number of covariates (\\(x_1, x_2\\)) on the response variable of interest (\\(y_i\\)). The \\(x_i\\) and \\(y_1\\) represent actual data and measurements we have gathered. The model used that information to estimate the relationships among those variables. The shape and direction of the relationship between a particular \\(x_i\\) and our response variable \\(y_i\\) is describe by the slope estimates (e.g., \\(\\beta_1, \\beta_2\\)). In this relatively simple example, the model estimates two other parameters. The model intercept, \\(\\beta_0\\) and the error term \\(\\epsilon_i\\). This type of model can contain multiple \\(x\\) variables and both the \\(y\\) and \\(x\\) values may be discrete (i.e, values are individual, separated and distinct) or continuous (i.e., unbroken values between a range). The nature of the data (i.e., continuous or discrete) will change how you plot the variables. For this example, we will work with sage-grouse lek data. One of the key questions driving much of wildlife ecology (honestly, much of most of ecology) is: how many animals are there? With the obvious follow up question of: why? Animal populations are influenced by a wide variety of environmental feature. Suppose we were interesting in determining if mean peak male counts (continuous \\(y\\) variable) were influenced by landscape or climate variables (continuous \\(x\\) variables) within the different management units (i.e., a discrete \\(x\\) variable) or within high and low nesting quality habitats (i.e., another discrete \\(x\\) variable). Before you begin with the code you need to load the lek data and add a grouping factor named “nest_type” which divides the leks into high (&gt;0.3) and low suitability (&lt;=0.3). This was covered in previous chapters. Unfortunately, in almost all field-base research, missing data is a reality. This is true of the sage-grouse data that we are using. Not all leks are surveyed every year.We will discuss how to deal with missing data in more depth in other parts of the course. For now, we want to remove leks from the analysis that have ‘too much’ missing data. We have chosen a 50% missing data as a cutoff for inclusion based on research by Dr. Fedy exploring the impact of missing data and frequency of counts on lek population dynamic estimates. Therefore, we create a new lek dataset for exploration and analysis, by removing any leks where no individuals were counted across the whole time period (i.e., inactive leks) and those with more than 50 percent missing data. We are not going to do any statistical analysis on this dataset for now, but below we use graphical analysis to test a number of the statistical assumptions outlined in Zuur et al. (2010). library(&#39;ggplot2&#39;) leks&lt;-read.csv(&quot;DATA/wysg_peak_males.csv&quot;,sep=&quot;,&quot;,header=TRUE) leks$nest_type&lt;-&quot;low&quot; # create a nesting suitablity variable leks[leks$mean_nest&gt;0.3,&quot;nest_type&quot;]&lt;-&quot;high&quot; # change rows with values &gt; 0.3 to &#39;high&#39; leks_sub&lt;-leks[leks$max_count != 0,] # subset with non-zero rows leks_sub&lt;-leks_sub[leks_sub$per_miss &lt; 50,] # subset for missing data 6.1 Outliers Statistical outliers (i.e., data points that are distant from other data points in the dataset) can dominate the results of statistical analysis. Thus, outliers should be identified prior to statistical analysis so they can be removed or fixed if proper justification is found (e.g., error in data entry) or their effects on the analyses can be quantified if they are true biological outliers. As in Zuur et al., 2010 we use box plots and Cleveland dot plots on our y variable. You will see below that although there are no egregious outliers, some may warrant attention. To create the 2-panel plot, we will use the cowplot package. Don’t forget to install and load the package before using the functions that require cowplot. Try and play around a bit with the plot_grid function, see how you can create different plot displays. # To make the two panel plot, we first assign the two plots to objects in R # Create the boxplot boxplot &lt;- ggplot(leks_sub, aes(y=mean_count))+ geom_boxplot() + labs(y=&quot;Mean peak males&quot;) + theme_bw() # Create the Cleveland dot chart dotplot &lt;- ggplot(leks_sub, aes(x=mean_count, y=seq(1, length(mean_count),1))) + geom_point() + labs(x=&quot;Mean peak males&quot;, y = &quot;Order of the data&quot;) + theme_bw() # Now plot them in the same window using the cowplot package #install.packages(&quot;cowplot&quot;, quiet=TRUE) # code for installing package library(&quot;cowplot&quot;, quietly=TRUE) cowplot::plot_grid(boxplot, dotplot) In the previous chapter we created multipanel plots using the facet_grid function. Here we used the cowplot to create a multipanel plot that displays the results from the two separate plots. Below we plot the mean peak male count and a number of landscape and climatic factors that are potentially having an effect on the number of males in a single plot. Before creating this plot, we create a subset matrix that can be used with the facet_wrap function. Once we create the new susbet matrix of data that only contains the mean_count and climate variables of interest, we will use code formatted based on the ‘tidyverse’, which uses pipes (%&gt;%), to format the dataset prior to plotting. The ‘tidyverse’ is a popular data science group of packages and coding style used to ‘tidy’ messy data to facilitate analysis and visualization. I use ‘tidyverse’ occasionally, but I am not a diehard convert. If you want to delve deeper into the ‘tidyverse’ I recommend the excellent book, R for Data Science which delves deep into the tidyverse universe and covers many important topics in data science. I take a pragmatic approach througout this book and the course. I do not really care what packages you use, as long as it works. library(&quot;tidyverse&quot;, quietly=TRUE) # create a matrix by binding only the columns we are interested in ex_data&lt;-cbind(leks_sub$mean_count,leks_sub$length_road,leks_sub$mean_sb, leks_sub$mean_herb,leks_sub$mean_nest,leks_sub$mean_ppt) # name the columns colnames(ex_data)&lt;-c(&quot;mean_count&quot;,&quot;length_road&quot;,&quot;mean_sb&quot;, &quot;mean_herb&quot;,&quot;mean_nest&quot;,&quot;mean_ppt&quot;) # create a new data set by converting the matrix to a data frame ex_data2 &lt;- ex_data %&gt;% as.data.frame() %&gt;% gather(key = &quot;variable&quot;, value = &quot;value&quot;) # plot the data ggplot(ex_data2, aes(x=value, y=seq(1, length(value),1))) + geom_point() + labs(x=&quot;Value of the variable&quot;, y = &quot;Order of the data&quot;) + theme_bw() + facet_wrap(~variable, scales = &quot;free&quot;) 6.2 Homoscedasticity (homegeneity of variance) An important assumption in many statistical analysis is that variance (i.e., spread of data points) is similar between groups (e.g., sexes, different experimental treatments). Essentially, this assumption means that the residuals from a model have equal variance (homoscedasticity) for every fitted value and the predictors. Meeting this assumption ensures accurate calculation of standard errors for the parameter estiamtes. Thus, if we were interested in determining if the mean peak number of males for leks was different depending on the management area or the surrounding categorical nesting quality it would be important to explore this assumption. Here we see the true value of these graphing packages as they easily allow us to compare the distributions within one or more grouping factors using a box plot. ggplot(leks_sub, aes(x=man_area, y=mean_count))+ geom_boxplot()+ labs(x=&quot;Management Area&quot;, y=&quot;Mean peak males&quot;)+ theme_bw()+ facet_wrap(~nest_type) Based on the plots above, it appears that we are violating the assumption of homogeneity of variance with some management areas having much lower variation in mean peak counts than others. We will discuss how to deal with this type of situation later in the course. But, these results suggest we may need to refine our research questions or account for these differences in our model development. Homoscedacity assumption determines how many variances are to be estimated: either one overall variance of the response variable, or several variances for different values of the explanatory variables. In the latter case, both the mean and the variance of the response variable change with the explanatory variables. We will get deeper into this when we discuss General(ized) Linear Models in later chapters particularly when we are discussing model fit. 6.3 Distribution of the response variable An assumption in many statistical analyses is that your y variable is normally distributed. A simple way to assess this assumption is to visualize your data using a histogram. Here we use a two panel plot to plot both the mean peak number of males and the log(peak number of males), which is a common statistical transformation used to closer approximate a normal distribution. hist_mean &lt;- ggplot(leks_sub, aes(x=mean_count)) + geom_histogram(bins=30) + labs(x=&quot;Mean number of males&quot;, y=&quot;Frequency&quot;)+ theme_bw() hist_log &lt;- ggplot(leks_sub, aes(x=log(mean_count))) + geom_histogram(bins=30) + labs(x=&quot;Log(Mean number of males)&quot;, y=&quot;Frequency&quot;)+ theme_bw() cowplot::plot_grid(hist_mean, hist_log, nrow=2) Again the lattice package can also be useful for plotting multi-panel plots with multiple variables. Here we use the grouping factor (nest_type) to look for normality within each nest group at the same time. ggplot(leks_sub, aes(x=mean_count))+ geom_histogram(bins=30)+ labs(x=&quot;Mean number of males&quot;, y=&quot;Frequency&quot;)+ facet_wrap(~nest_type)+ theme_bw() 6.4 Collinearity among x variables When x variables (i.e., predictors) are highly correlated it is difficult to determine their independent effect on the y variable. Thus, it is always a good idea to determine the level of correlation among your predictors. Here we use a scatter plot to visualize the collinearity between the three landscape variables in the ex_data dataframe. Here will use the ggscatmat function from the GGally package to visualize the correlation between variables. You will also notice in the plot that there is a strong positive correlation between sagebrush habitat and nesting quality, which is not surprising given the ecology of sage-grouse and their association with this habitat type. #install.packages(&quot;GGally&quot;, dependencies=TRUE, quiet=TRUE) library(&quot;GGally&quot;, quietly=TRUE) ## Registered S3 method overwritten by &#39;GGally&#39;: ## method from ## +.gg ggplot2 ggscatmat(ex_data, columns = c(3,4,5)) #columns function selects specific comparisons "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
