[["index.html", "Applied Statistics in Environment and Ecology Chapter 1 Welcome", " Applied Statistics in Environment and Ecology Dr. Brad Fedy 2025-01-04 Chapter 1 Welcome This book is being written to support student progress and learning in ERS 669 Applied Statistics in Environment and Ecology. Notice the present tense in the previous sentence. This book is a work in progress. The book is not intended for distribution, is currently missing important references (and cross-references), and frequently relies on the information presented during lecture periods for proper interpretation. The book is written using Bookdown which is a package in R that facilitates writing books with R Markdown. The book is provided as .HTML files on LEARN. This allows for a dynamic document allowing you to easily navigate through book sections and copy code. The book files will be updated on LEARN each week in preparation for our in-class programming sessions. "],["introduction-to-r.html", "Chapter 2 Introduction to R 2.1 What is R? 2.2 RStudio 2.3 Before you begin 2.4 Setting a working directory 2.5 Getting help in R 2.6 Defining R objects 2.7 Exporting data frames", " Chapter 2 Introduction to R 2.1 What is R? R is a programming language and software environment for statistical computing and graphics created by Ross Ihaka and Robert GentlemanIhaka R. &amp; Gentleman R. 1996. R: a language for data analysis and graphics. Journal of Computational and Graphical Statistics 5: 299-314. The development and distribution of R is carried out by a group of statisticians known as the R Development Core Team and is distributed under the terms of the GNU Public License. Thus, R can be freely downloaded from the Comprehensive R Archive Network (CRAN) website. The fact that R is free and available across operating systems is one of the main benefits of learning and using R for statistical analysis. There are multiple other freely available programming languages and software what can be used for statistics and data science (e.g., Python), but R is one of the most common for applications in academia and in ecology and environmental studies, specifically. Additionally, there are many commercial statistical packages that are comparable and can be used to similar ends; but, at a cost &gt; $1,000 per license. Installation of R software is straight forward for all major platforms and instructions are provided on the website. Most of the analyses and tasks that are conducted in R are done through the command-line interface and thus, for the non-programmer the learning curve for this software may be steeper than for most point and click statistical programs. There are, however, several advantages of using R and the command-line setup that make putting the effort into learning this program well worth your time: Cost: It’s free! No matter what lab, agency, or company you work with, R can go with you. Simplicity: R is a relatively simple programming language, allowing you to accomplish a lot with minimal lines of code. Diverse and continued package development: In the past, researchers have needed to learn multiple software packages to conduct their analyses and display their results. Because R is open source and relies on user contributed packages, a wide variety of general and specialized analyses and plotting functions can be conducted in R. New packages are developed and existing ones are updated each day. This reduces this need to learn multiple software packages. Integration: R works nicely with other scripting languages and open source software used for a variety of tasks, including Geographic Information Systems (e.g., GRASS, QGIS) and document production (e.g., Markdown - more on this later). Reproducibility: If used properly, command-line scripting keeps detailed records of your analyses. This makes is easy to share, repeat, and modify your work. This is an important point. One of the most important components of good science is replicability. Most research requires many steps from organizing and checking the data to conducting the analysis. Using statistical software that allows you to point and click through your analysis does not usually leave any record of what has been done. Thus, it can be difficult to remember and precisely replicate the data manipulation and analyses. Using command-line scripting requires the analyst to type out a line of code for every action they want to perform. Use of a code hosting platform such as GitHub can further facilitate transparency, reproducibility, version control, and collaboration. We will not cover the use of GitHub explicitly in this course, but it is a potentially useful tool you may encounter as you progress through your research. Self-education: There is a large, active, and (mostly) helpful community of people dedicated to education and training in R programming languages. With enough effort you can find online tutorials or examples for any statistical analysis in R, along with example code. This community is one that you will rely on throughout your R programming and statistical education. This manual will serve multiple purposes. It will be used to provide examples during lectures and code that you will be required to work through on your own. The introduction to R will be brief so that we can get into working with data as quickly as possible. However, there are many excellent resources to facilitate your R-based education. Do not fall behind in your understanding of R. Ask frequent questions of Dr. Fedy and your colleagues. Be sure you understand each line of code. We will use R for all course topics. It is essential that you understand the code in order to understand the data management and statistics that are the focus of the course. This manual is not intended to turn you all into R programming ninjas and statisticians. The intention is to provide you with the tools required for self-education. R is a potentially relevant tool for anyone who uses numbers to understand the world - regardless of your area of interest. When programming in R, there are often multiple ways to solve the same problem. That is part of the fun and challenge of programming. Of course, the different options to completing an analysis increase with increasing complexity of the problem; for simple problems there are usually only a few ways to solve it, for complex problems there are often many ways to solve it. Throughout the course I will demonstrate the code and approaches that have worked for me, my students, and collaborators. However, I intentionally do not provide all the answers and you will need to problem-solve and self-educate in order to successfully complete all the assignments. You will need to install two pieces of software on your computer: R and RStudio. Both of these are free to download. When you open R, you will see the basic command-line console version of R. RStudio is an Integrated Development Environment (IDE) for R, that provides a few more bells and whistles and tends to ease the introduction to R. You do not have to use RStudio, but it is highly recommended. 2.2 RStudio One of the most significant advantages of using the command-line structure in R is that, if done properly, you have detailed documentation for everything that you do. This allows you to easily change, manipulate, and share your code with others. This process is facilitated by writing any code you use into a script file, instead of entering it directly into R. Rstudio can be customized in many ways, but is typically divided into four windows and that provide simultaneous views of important information. The most frequently used windows and tabs are: scripts: Write your code here for a permanent record. console: Essentially R running within RStudio. Displays output and any errors. Environment/Plots: Environment shows you the objects that are currently saved in your workspace and Plots displays any plots that you create. History/Files/Packages: These tabs are less commonly referenced and are fairly self-explanatory. 2.3 Before you begin There are four important things that you should note about R and this manual that will help you as you work through the instructions and exercises: Throughout this book R code is in the grey boxes, with results that you should see in R following this text prefaced by ##. Text following a # are not read by R and are used to comment the code. I use # marks to provide information about a particular command or function. When working in the R console, the up and down arrows can be used to retrieve past commands, saving you from re-typing it. If you see a + prompt, it means R is waiting for more input. Often this means that you have forgotten a closing parenthesis or made some other syntax error. If you cannot figure out its meaning, hit the escape key and start the command fresh. In R, tasks such as calculating means, conducting statistical analysis or generating plots are completed through the use of functions. A function in mathematics is something that relates an input to an output. It has three basic components an input, a relationship, an output. The function defines the relationship. The arguments are the variables or inputs required for the function to produce the desired output. For example, a function to calculate the mean height of everyone in the class would require 1 argument: the height of each person. The function calculates the number of people (n) based on the number of observations and outputs the mean by summing all individual heights and dividing by n. We will go through many examples of functions throughout the course. Functions in R accept arguments, which are used to complete the task (produce output) and use the following syntax: &gt; functionname(argument 1, argument 2, ...) The arguments are always surrounded by (round) parentheses and separated by commas. Some functions (e.g., data()) have no required arguments, but you still need the parentheses. If you type a function name without the parentheses, you will see the code that is the basis for that function including the required arguments. Before you begin any analysis it is important that you tell R the home directory that you will be working in by using the command setwd(“path to folder”). It is important that you include the quotation marks (“”) around the pathname. It is also important that you have write access to the drive that you are using for your working directory. Please keep this in mind if you are using multiple computers. You can set your working directory using the drop down menus in RStudio, by typing out the path manually, or by using the command file.choose(). 2.4 Setting a working directory The first steps for setting a working director typically happen outside of R or RStudio. The first step is to create a new folder on your computer where you want to store all of the work associated with this class or with a particular assignment. Once the folder is created, the second step is to put a file into the folder you created. It does not matter what type of file you put in this folder. Typically, I use a simple blank .txt or the data file (typically .csv) file for examples in class. Once this is complete navigate back to R and type in the following command at the prompt: file.choose() R will then open your computers file management and navigation system. For Mac OS this is Finder, in Windows machines, the Explorer window will open. Within that window, navigate to the working folder you just created and the file you put in it. Then select (“choose”) that file. R will respond by putting the entire path name into your R console window. [1] &quot;/Users/bfedy/Documents/PROJECTS/Rwork/Anything filename.txt&quot; Select everything in the path except for the file name (e.g., the .txt file) and copy and paste it into your setwd command. setwd(&quot;/Users/bfedy/Documents/PROJECTS/Rwork/&quot;) Once this is completed R will put everything it generates and look for any files you pass to it in the “Rwork” file that I created above. Please note: the path names will be different on every computer that you use, so you will need to reset the working director every time you change computers. 2.5 Getting help in R The list of functions available within R is much too great to provide even an adequate summary. There are, however, a wide variety of online resources available that we expect you to draw on throughout this course. Searching “R” and “Type of analysis” in a search engine will often produce code and sometimes tutorials for the analyses you are looking to undertake. To be more specific, you can also include “cran” as a keyword. As you work through this manual and your assignments I encourage you to start a document where you note commands that you find useful with a brief description. This will help save you from learning the same command multiple times. You are learning a new language, please take notes and do not expect yourself to remember everything. You might want to check out some cheat sheets compiled by R as examples. There are also a variety of ways to get help within the R software. If you type a ? followed by the command or help(commandname) will pull up a help file document with the follow structure: Description: Brief description. Usage: For a function, gives the name with all its arguments and the possible options (with the corresponding default values); for an operator gives the typical use. Arguments: For a function, details each of its arguments. Details: Detailed description. Value: If applicable, the type of object returned by the function or the operator. See Also: Other help pages close or similar to the present one. Examples: Some working examples of the function or operator. Other methods for obtaining help when you do not know the exact name of the command you are looking for is to use ?? or help.search with a key word. This will give a list of commands with topics that match your key word. Expect to spend a good amount of time searching for help on the internet while you are programming - particularly at the beginning. It is common to spend as much time (or more) searching the internet as you spend writing code when first starting out. There are many excellent websites and free resources for help and guidance through your R and statistical journey. I cannot list them all here; however, two books that I go back to regularly for R programming help are: R for Data Science R Graphics Cookbook 2.6 Defining R objects A fundamental concept of R is that when R is running, data and results are stored in the active memory in the form of objects, which are all assigned a name by the user. The user can then manipulate these objects through the use of operators or functions. Operators perform tasks including arimetic, logical, and conditional actions. Functions are the work horses of R and are self-contained modules of code that accomplish a specific task. Functions are typically provided data (i.e., values, vectors, dataframes), process that data according to the rules defined in the function, and return a result. Functions can be user-generated, but the majority of functions that you use will be contained within R packages. Much more information will be presented on both these topics throughout the book. Objects can be stored in a variety of ways and it is critical to understand how objects are created, their formats, and what they mean as a first step in learning R. The most basic form of an object is a scalar (i.e., single value), which can be creating using the “assign” &lt;- operator. The assign operator is simply a less than sign followed by a minus sign. The following examples demonstrate the basics. n &lt;- 15 n ## [1] 15 n &lt;- &quot;Applied Statistics&quot; n ## [1] &quot;Applied Statistics&quot; N &lt;- 18 Two things of particular note in the example above, 1) R is case sensitive with n being different than N, and 2) if an object exists and is assigned a new name, the previous value is erased without any warning. Thus, it is important to pay attention to the variables you create and those which are already stored in your active memory. You can get a list of the names of variables in memory with the ls() command or for the variables with some details on the objects use ls.str(). In RStudio, you can also see all the objects you have created in the ‘Environment’ tab. To remove objects from the active memory you can use the rm('object') command. Look at the example below and note that we use rm(list=ls()) to list and remove all objects in the active memory. It is good practice to start all of your new scripts with the rm(list=ls() command. That way you know the script is not dependent on any objects previously stored in your Environment (i.e., working memory). When you run the code below, you will see the corresponding values show up in R console. If you want to save those values for later use, you will need to assign them an object name. Y &lt;- c(2,2,3,5,6,7,10,11,11,12) # create a vector of these numbers Y[4] # select the fourth value ## [1] 5 Y[1:5] # select the first five values ## [1] 2 2 3 5 6 Y[5:10] # select the last five values ## [1] 6 7 10 11 11 12 To select or identify values within two dimensional data such as matrices and data frames you will need two numbers (separated by a comma) in the square brackets. The first number specifies the row or range of rows, and the second number specifies the column or range of columns (Dr. Fedy finds it helpful to think of RC Cola to remember the order: rows, then columns. You may not have heard of RC Cola, but you get the point, think of something meaningful to you). For data frames you may also want to extract the column headers of names of columns, which you can replace if you wish to change the name of one or more columns. Below are some examples executed using base R. There are multiple packages that make the selection of data and values more intuitive (much more on those later). # Set up a 2 dimensional data frame options(digits = 3) # set significant digits in R snake.df &lt;- data.frame(species = rep(&quot;P.gloydi&quot;, 20), sex = factor(rep(c(&quot;male&quot;, &quot;female&quot;),10)), mass = runif(n=20, min=0.5, max=1.5), length = runif(n=20, min=50, max=100)) snake.df ## species sex mass length ## 1 P.gloydi male 1.476 52.7 ## 2 P.gloydi female 1.068 96.3 ## 3 P.gloydi male 0.897 85.0 ## 4 P.gloydi female 1.263 85.8 ## 5 P.gloydi male 0.887 66.4 ## 6 P.gloydi female 1.204 66.7 ## 7 P.gloydi male 1.209 86.3 ## 8 P.gloydi female 0.903 81.8 ## 9 P.gloydi male 0.841 66.3 ## 10 P.gloydi female 1.449 93.7 ## 11 P.gloydi male 0.840 54.5 ## 12 P.gloydi female 0.541 68.9 ## 13 P.gloydi male 0.837 82.2 ## 14 P.gloydi female 0.610 89.5 ## 15 P.gloydi male 1.333 65.2 ## 16 P.gloydi female 0.798 68.7 ## 17 P.gloydi male 1.335 85.4 ## 18 P.gloydi female 0.509 75.0 ## 19 P.gloydi male 1.380 82.1 ## 20 P.gloydi female 0.862 60.1 Be sure that you understand what is happening in each of the lines of code above. This would be a good time to use the help functions in R to help you understand. # The output from the following code will show up in your R console names(snake.df) # extract names of columns ## [1] &quot;species&quot; &quot;sex&quot; &quot;mass&quot; &quot;length&quot; snake.df[ , 1] # extract the first column (note: the space are not necessary) ## [1] &quot;P.gloydi&quot; &quot;P.gloydi&quot; &quot;P.gloydi&quot; &quot;P.gloydi&quot; &quot;P.gloydi&quot; &quot;P.gloydi&quot; ## [7] &quot;P.gloydi&quot; &quot;P.gloydi&quot; &quot;P.gloydi&quot; &quot;P.gloydi&quot; &quot;P.gloydi&quot; &quot;P.gloydi&quot; ## [13] &quot;P.gloydi&quot; &quot;P.gloydi&quot; &quot;P.gloydi&quot; &quot;P.gloydi&quot; &quot;P.gloydi&quot; &quot;P.gloydi&quot; ## [19] &quot;P.gloydi&quot; &quot;P.gloydi&quot; snake.df[1, ] # extract the first row ## species sex mass length ## 1 P.gloydi male 1.48 52.7 snake.df[ , 1:3] # extract the first 3 columns ## species sex mass ## 1 P.gloydi male 1.476 ## 2 P.gloydi female 1.068 ## 3 P.gloydi male 0.897 ## 4 P.gloydi female 1.263 ## 5 P.gloydi male 0.887 ## 6 P.gloydi female 1.204 ## 7 P.gloydi male 1.209 ## 8 P.gloydi female 0.903 ## 9 P.gloydi male 0.841 ## 10 P.gloydi female 1.449 ## 11 P.gloydi male 0.840 ## 12 P.gloydi female 0.541 ## 13 P.gloydi male 0.837 ## 14 P.gloydi female 0.610 ## 15 P.gloydi male 1.333 ## 16 P.gloydi female 0.798 ## 17 P.gloydi male 1.335 ## 18 P.gloydi female 0.509 ## 19 P.gloydi male 1.380 ## 20 P.gloydi female 0.862 snake.df[1:4,1:3] # extract the first 3 columns and 4 rows ## species sex mass ## 1 P.gloydi male 1.476 ## 2 P.gloydi female 1.068 ## 3 P.gloydi male 0.897 ## 4 P.gloydi female 1.263 It is also possible using base R to call individual columns of a data frame using the names of the data frame and the column separated by a $. snake.df$sex ## [1] male female male female male female male female male female ## [11] male female male female male female male female male female ## Levels: female male snake.df$length ## [1] 52.7 96.3 85.0 85.8 66.4 66.7 86.3 81.8 66.3 93.7 54.5 68.9 82.2 89.5 65.2 ## [16] 68.7 85.4 75.0 82.1 60.1 2.7 Exporting data frames As we alluded to earlier, in reality R’s main function is for analyzing and graphing data, and different programs such as excel or other database programs will be used when collecting and entering data. Thus, being able to import and export data from R is essential. Here, we will first introduce you to the write.table() function, which is the primary function used to export a data fame. The main parameters that you need to be aware of are listed below, but you can use R help ?write.table to get a list of all options and more information. One of the most important arguments in this function is the sep argument, which specifies how each column in the data frame will be separated. As a default, I tend to prefer the sep=\",\" option which allows you to open your data a comma separated file (.csv) which is possible in most editors that you will want to use. If you do not provide a file name (as we do below) for the argument file then the output is printed to the screen. After you create the table below try opening it in a text editor or Excel (or some other spreadsheet software). Be sure you have set your working director setwd(\"filename\") so you know where the output will be stored on your machine. write.table(snake.df, sep=&quot;,&quot;, row.names=FALSE, col.names=TRUE) 2.7.1 Installing and loading packages An important component and one of the main advantages of R is that users can write and provide packages. These packages are essentially a series of functions that have been compiled together and can be loaded into R to make these functions available. Without installing and loading these packages these functions are not available. Loading packages in R can be done in R from the command-line if you know the package that you wish to install. Two packages that we will need in later models are the lattice and grid packages. These can be installed using the input below, with dependencies=TRUE specifying that you wish all packages that are needed to be installed at the same time. You may get a pop up window asking if you want to install these in a local directory. If so you should answer yes. You will likely also be asked to specify a CRAN mirror (i.e., download location), so just choose Canada (ON). install.packages(&quot;lattice&quot;, dependencies = TRUE) install.packages(&quot;gridBase&quot;, dependencies = TRUE) In addition to installing packages through the command line you can search for an install packages using the ‘Packages – Install Packages’ within RStudio. Once packages are installed, they must also be loaded locally before you can use the functions contained within the packages. You load packages with the library(\"packagename\") command (replace packagename with the name of the package you are loading). When you have a number of packages to load, I copy, paste and revise, the following code. It is a very efficient script to have at the beginning of your code. The original code was provided to me by Dr. Chris Beirne who has written an excellent book on Camera Trap Data Management and Analysis in R. # Load packages #-----------------------------------------------------------------------------. list.of.packages &lt;- c(&quot;lattice&quot;, &quot;gridBase&quot;) # Check to see packages are missing new.packages &lt;- list.of.packages[!(list.of.packages %in% installed.packages()[,&quot;Package&quot;])] # Install the missing packages and load if(length(new.packages)) install.packages(new.packages) lapply(list.of.packages, require, character.only = TRUE) "],["operators.html", "Chapter 3 Operators 3.1 Manipulating R objects", " Chapter 3 Operators 3.1 Manipulating R objects In the previous chapter, I summarized the main types of R objects and how these can be stored in the active memory. Most analyses and tasks that you will complete in R will require you to manipulate these objects and this can be done through a variety of methods. Most of these, however, will require the use of R operators, which are used to manipulate or subset all types of R objects. There are three basic categories of operators that you should be aware of and are summarized below. ARITHMATIC Operator Description + addition - subtraction * multiplication / division COMPARISON Operator Description &gt; greater than &lt; less than &gt;= greater than or equal to &lt;= less than or equal to == equal to != not equal to LOGICAL Operator Description X &amp; Y and (X and Y) X|Y or (X or Y) ! X not (not X) The simplest R operators are basic arithmetic, which basically allows R to act as a calculator. A couple things to note in the examples below, 1) The results of an arithmetic expression can be stored in memory by assigning the result to an object (for simplicity we do not do this for many of the examples) and, 2) when applying operators to vectors or matrices the operation will be conducted on each element in the vector (not the case for data frames where you must specific a column). 2 + 2 # addition ## [1] 4 45*90 # multiplication ## [1] 4050 sqrt(81) # calculate the square root ## [1] 9 x &lt;- 2 + 2; y &lt;- 45*90; z &lt;- sqrt(81) x; y; z ## [1] 4 ## [1] 4050 ## [1] 9 x &lt;- c(20:30) x ## [1] 20 21 22 23 24 25 26 27 28 29 30 x * 10 ## [1] 200 210 220 230 240 250 260 270 280 290 300 The output in each of the above examples above begins ## [1]. The ## was explained in the previous Chapter and exists at the beginning of output in a bookdown document so that it will not run if you copy and paste it into R. The [1] is used to index a value in a vector. For example in the vector x that we created above the first value is 20 and is indexed by [1] if we want the 3rd value in the vector we can retrieve it by using the indexing as follows: x[3] ## [1] 22 Vector arithmetic is an important concept and will be essential when working with real data and you want to manipulate specific columns using an operator. Below we run through a series of examples where this is the case. Be sure to check the objects and output after each line of code. If you do not understand why you got the output you did please ask. It is essential that you understand these actions. They are the foundation. If you have only a vague understanding of what is happening at this point, it will make all subsequent assignments much more difficult. Simply copying from this manual without examining the output will teach you little about R. Typically, we enter our data into a .csv file and import that data into R (more on this later). For example purposes, we will create a basic simulated dataset using code. Consider the following data frame of the widths and lengths of snakes: options(digits = 3) # set significant digits in R snake.df &lt;- data.frame(species = rep(&quot;P.gloydi&quot;, 20), sex = factor(rep(c(&quot;male&quot;, &quot;female&quot;),10)), mass = runif(n=20, min=0.5, max=1.5), length = runif(n=20, min=50, max=100)) snake.df ## species sex mass length ## 1 P.gloydi male 0.943 97.5 ## 2 P.gloydi female 0.590 90.0 ## 3 P.gloydi male 0.503 71.0 ## 4 P.gloydi female 1.157 61.8 ## 5 P.gloydi male 1.283 65.5 ## 6 P.gloydi female 1.008 91.0 ## 7 P.gloydi male 0.936 74.1 ## 8 P.gloydi female 0.528 77.4 ## 9 P.gloydi male 0.809 81.7 ## 10 P.gloydi female 1.349 70.1 ## 11 P.gloydi male 0.944 68.5 ## 12 P.gloydi female 0.823 66.8 ## 13 P.gloydi male 1.365 85.7 ## 14 P.gloydi female 1.141 91.7 ## 15 P.gloydi male 1.299 54.2 ## 16 P.gloydi female 0.856 64.1 ## 17 P.gloydi male 1.271 61.5 ## 18 P.gloydi female 0.783 59.1 ## 19 P.gloydi male 1.044 62.3 ## 20 P.gloydi female 1.391 55.1 Suppose you want to convert the weight of the snakes from kilograms to grams (i.e., multiply by 1000) or create a new variable body condition, which is the weight divided by the length. Both of these can be easily done using an arithmetic operator. Recall, that if you want to call a specific column you simply use the data frame name and the column name separated by a $. If this column exists it will be replaced and if it is does not it will be created. # Replace mass column with mass in kg snake.df$mass &lt;- snake.df$mass*1000 snake.df$body.cond &lt;- snake.df$mass/snake.df$length snake.df ## species sex mass length body.cond ## 1 P.gloydi male 943 97.5 9.68 ## 2 P.gloydi female 590 90.0 6.55 ## 3 P.gloydi male 503 71.0 7.08 ## 4 P.gloydi female 1157 61.8 18.73 ## 5 P.gloydi male 1283 65.5 19.57 ## 6 P.gloydi female 1008 91.0 11.08 ## 7 P.gloydi male 936 74.1 12.64 ## 8 P.gloydi female 528 77.4 6.82 ## 9 P.gloydi male 809 81.7 9.90 ## 10 P.gloydi female 1349 70.1 19.25 ## 11 P.gloydi male 944 68.5 13.79 ## 12 P.gloydi female 823 66.8 12.32 ## 13 P.gloydi male 1365 85.7 15.93 ## 14 P.gloydi female 1141 91.7 12.45 ## 15 P.gloydi male 1299 54.2 23.96 ## 16 P.gloydi female 856 64.1 13.37 ## 17 P.gloydi male 1271 61.5 20.65 ## 18 P.gloydi female 783 59.1 13.24 ## 19 P.gloydi male 1044 62.3 16.75 ## 20 P.gloydi female 1391 55.1 25.23 A second important class of operators are the comparison operators, which are often used to subset your data in meaningful ways. For example, consider a situation that requires you subset the snake data by sex and you want only female snakes. Here you would use the == operator, combined with the square brackets [ ]. The square brackets were introduced above for indexing a vector. They can also be used to index matrices and dataframes (i.e., 2-dimensional objects…and beyond). Which we know from earlier can be used to subset your data. In this case however, instead of using numbers to select columns, we will use an operator to select rows that are equal to “male”. You can also use other comparison operators such as greater than ‘&gt;’ or less than ‘&lt;’. Finally, we combine a comparison with a logical operator. Remember that when subsetting with the [ ] the first value indicates rows and the value after the “,” indicates columns. When you want all columns, leave the number after the “,” blank, and leave the number before the “,” blank when you want all rows. "],["data.html", "Chapter 4 Data 4.1 Writing scripts 4.2 Importing, viewing, and editing data", " Chapter 4 Data 4.1 Writing scripts When writing R scripts there are a couple things to keep in mind that will make your life much easier. Future you, will thank present you. Most of the time you will write your scripts in an R script file and then send your commands to the R Console from that file. Always start your code with a text header that provides information about your script file so that future you (and others) know what the script does and can reproduce your analysis. At a minimum I suggest the following: Brief description of the script (i.e., a title) The date the script was created or last modified (there are more elegant ways to keep track of this that we will address later) The author of the script R version used when creating the script (R packages change. This is important.) Other items that I ask everyone in my research group to include are: Packages required to run the code (there are more elegant ways to keep track of this that we will also address alter) Data inputs and sources Data outputs Here is an example of the Header text I try to include in all my scripts. Obviously, not all this information is available at the beginning of an analysis, but I encourage you to update your header as you go. #=============================================================================# # HEADER --------------------------------------------------------------------- # # Author: Brad Fedy # # R.Version(): Version 4.3.1 # # Project: # Grassland birds habitat selection # # Required libraries: # Mumin, data.table # # Required input files: # 1. Bird location data e.g., &quot;species.routes.SPPI.csv&quot; # 2. Covariate values at each scale creaeted in step _3.. Example naming: # &quot;SPPI.covariates.`scale&#39;.csv&quot; # # Steps: # 1. PREPARE DATA FOR MODELLING # 2. RUN UNIVARIATE MODELS # 3. CORRELATIONS AMONG TOP COVARIATES # 4. UNIVARIATE MODELS FOR ALL CORRELATED PAIRS # # Output files: # 1. Table of univariate results across all scales for each variable. # e.g., &quot;SPPI.model.result.&quot;,env[[s]],&quot;.csv&quot; # 2. Table of univariate results of each highly correlated pair # e.g., &quot;SPPI.cor.var.models.&quot;,x,&quot;.csv&quot; where `x&#39; is a number 1 to n pairs # # Date: 2023/05/13 # # NOTES: #=============================================================================# rm(list=ls()) setwd(&quot;/Users/bfedy/grassland.birds/DATA/Bird.Data&quot;) library(&quot;MuMIn&quot;) library(&quot;data.table&quot;) The formatting of your scripts can be a matter of personal preference. There are a number of style guides available. Hadley Wickham has one associated with his Advanced R book and Google’s R Style Guide can be found in multiple locations on the internet. Basically, try to make it readable, easily understood, and do not be shy with your use of # comments throughout. You will be grateful later. Really…. more comments are almost always better. 4.2 Importing, viewing, and editing data Greater Sage-Grouse (Centrocercus urophasianus; hearafter sage-grouse) are ground nesting bird, distributed across the intermountain west with a range that crosses 11 states and 2 Canadian provinces. They are listed under the Canadian Species At Risk Act and of conservation concern throughout their distribution in the United States. Their distribution is roughly consonant with the distribution of sagebrush (Artemisia sp.) habitat. The major negative impacts to sage-grouse are habitat loss, invasive species, and energy development across their range. Sage-grouse have a lek mating system in which males congregate at breeding sites (i.e., leks) each spring to compete for mating opportunities using elaborate ‘strutting’ displays to attract females. There have been large efforts across their range to collect detailed ecological and population data - typically focused on lek sites. Approximately 35% of the remaining sage-grouse occur in Wyoming. Long term monitoring data exists for many of the active leks across Wyoming which are visited 1-3 times per lek season by biologists who count the number of males displaying at a lek. By convention, the highest number of males recorded across all visits is used as the lek size for a given year. These counts are considered a good index of overall population size and are used to assess trends in population numbers. This ongoing effort was started in 1948 and has resulted in a large dataset with thousands of leks where males have been counted and recorded each year. We will use some of these data to demonstrate approaches that can be used to visualize and analyze a real ecological dataset. Given issues with data ownership and distribution, the data you will be working with are based on real data, but I have adjusted the data by introducing random variation into the counts and spatial lek locations. So the actual data you are working with have similar distributions and challenges associated with the real data, but but are not accurate in terms of lek locations and lek counts. Th To get you started, we have provided a comma separated values file, which includes summary information on 718 leks from the southern half of the Wyoming. In the figure below you can see distribution of these leks in relation to their range (light grey) across this region. We have summarized the time-series of the leks described and mapped above, by calculating a variety of statistics on the time series form 1982-2013 (“241223wysg_leks.csv”). These data are based on empirical data, but all of the locations and counts have had substantial error introduced so they are not “true” data. This was done due to the sensitivity of the data. Before beginning, get associated with this data by opening it in a text editor and excel. A description of data comprised for each column is listed below. Name Description lekid name of lek x UTM easting coordinates y UTM northing coordibnates area name of the management ares (i.e., unit) per_miss percent of years where lek counts were not conducted per_zero percentage of years where no individual males were counted at a lek site mean_count mean number of peak males counted across the time series sd_count standard deviation of peak males counted across the time series min_count minimum number of peak males counted across the time series max_count maximum number of peak males counted across the time series len_road length (m) of raods within 5 km radio of lek mean_ppt mean of July, August, and September 1981-2010 total precipitation for climate zone surrounding lek mean_sage mean sagebrush percent cover within 5 km radius of lek mean_herb mean herbaceous percent cover within 5 km radius of lek mean_nest mean nesting habitat suitability for sage-grouse (range: low(0) to high(1)) within 5km radius of lek Once you are familiar with the data table, open Rstudio and setup a script file (do not forget to save the script file) that you will use to enter your commands throughout all exercises. You can translate your commands from the script to the concole (i.e., run your commands), by highlighting the commands and hitting control-enter on the keyboard. Make sure you input header information before you begin entering commands! It is also important to set your working directory to the directory where your data is saved before attempting to import the data. # set your working directory leks &lt;- read.table(&quot;DATA/241223wysg_leks.csv&quot;, sep=&quot;,&quot;, header=TRUE,quote=&quot;&quot;,comment.char=&quot;&quot;, stringsAsFactors=FALSE) # read in the data head(leks) # look at the first few observations ## lekid x y area per_miss per_zero mean_count sd_count ## 1 D-Alkali Draw -1117044 2253178 D 46.9 50.0 34.529 17.6 ## 2 D-Antelope Draw -1154827 2281868 D 28.1 40.6 52.391 45.4 ## 3 D-Bench Corral -1148670 2278415 D 25.0 43.8 33.583 28.6 ## 4 D-Big John -1099343 2250458 D 50.0 50.0 87.875 19.1 ## 5 D-Big Sandy Flat -1088257 2252134 D 25.0 90.6 0.875 3.0 ## 6 D-Billie&#39;s Draw -1152847 2275456 D 50.0 71.9 20.062 24.6 ## min_count max_count len_road mean_ppt mean_sage mean_herb mean_nest ## 1 0 67 172473 24.8 14.8 15.9 0.484 ## 2 0 133 188797 28.2 16.5 16.7 0.525 ## 3 0 87 219941 25.0 14.7 13.8 0.463 ## 4 56 119 134197 24.9 14.5 14.3 0.434 ## 5 0 14 185886 25.6 16.1 17.0 0.487 ## 6 0 73 171083 26.5 16.0 13.4 0.555 There are often issues when importing data. Typically these are caused by variables that contain character data. You can visually inspect your imported data using the view() function or by clicking the data table in your Environment in RStudio. It is useful to always use summary functions to ensure that everything imported as expected. Here we use two such functions. dim(leks) # provide dimensions of the data frame ## [1] 718 15 summary(leks) ## lekid x y area ## Length:718 Min. :-1236354 Min. :2043637 Length:718 ## Class :character 1st Qu.:-1129942 1st Qu.:2122548 Class :character ## Mode :character Median : -980625 Median :2166752 Mode :character ## Mean :-1012654 Mean :2169603 ## 3rd Qu.: -910777 3rd Qu.:2208761 ## Max. : -790076 Max. :2319509 ## per_miss per_zero mean_count sd_count min_count ## Min. : 0.0 Min. : 0.0 Min. : 0.0 Min. : 0.0 Min. : 0.0 ## 1st Qu.: 0.0 1st Qu.: 46.9 1st Qu.: 2.6 1st Qu.: 5.8 1st Qu.: 0.0 ## Median :12.5 Median : 65.6 Median : 10.3 Median : 11.7 Median : 0.0 ## Mean :19.9 Mean : 62.2 Mean : 16.6 Mean : 15.6 Mean : 1.2 ## 3rd Qu.:34.4 3rd Qu.: 81.2 3rd Qu.: 23.8 3rd Qu.: 20.8 3rd Qu.: 0.0 ## Max. :81.2 Max. :100.0 Max. :159.4 Max. :104.5 Max. :80.0 ## max_count len_road mean_ppt mean_sage mean_herb ## Min. : 0 Min. : 35552 Min. :15.8 Min. : 0.00 Min. : 3.8 ## 1st Qu.: 22 1st Qu.:122144 1st Qu.:20.9 1st Qu.: 8.00 1st Qu.: 8.2 ## Median : 42 Median :155570 Median :23.9 Median : 9.99 Median :11.5 ## Mean : 56 Mean :160325 Mean :24.4 Mean :10.19 Mean :12.8 ## 3rd Qu.: 77 3rd Qu.:193281 3rd Qu.:27.4 3rd Qu.:12.27 3rd Qu.:16.3 ## Max. :350 Max. :396330 Max. :35.8 Max. :19.76 Max. :51.2 ## mean_nest ## Min. :0.000 ## 1st Qu.:0.101 ## Median :0.162 ## Mean :0.198 ## 3rd Qu.:0.262 ## Max. :0.695 It is useful to check the format of your data. We previously discussed some of the potential issues of storing columns as factors. Check to see if any columns are stored as factors. If needed re-import the data so that no columns are factors. 4.2.1 Summarizing and manipulating data One of the key components to any analysis is the ability to summarize and manipulate your data. There are almost as many ways to accomplish common data summary tasks as there are users. We provided some of these basic functions earlier. However, in many cases you will want to summarize a data set using a grouping variable. For example, suppose we wish to determine the mean number of males counted within each of the management areas. One way this can be done using base R is with the tapply() function which applies a function to each group within a vector. To use tapply you supply i) a numeric vector of a data frame to be summarized (X; i.e., your variable of interest), ii) a grouping factor (INDEX), and iii) a function (FUN) to summarize the numeric vector: tapply(X=leks$mean_count,INDEX=leks$area,FUN=mean) ## D E F G H ## 30.96 28.49 8.53 16.68 13.14 This example used mean; however, tapply can be used with any built in function (e.g., sum(), sd(), length(), max(), min()) or with a customized function of your own design. In addition to this flexibility, more than one grouping factor can be incorporated. For example, suppose we had some idea of a breakpoint between high and low quality nesting habitat for leks and we wanted to calculate a summary table describing the mean male counts for low and high suitability leks within each management area. To accomplish this we first need to create an additional variable, which groups the leks into ‘low’ or ‘high’ suitability groupings. leks$nest_type&lt;-&quot;low&quot; # create a nesting suitability variable leks[leks$mean_nest&gt;0.3,&quot;nest_type&quot;]&lt;-&quot;high&quot; # change rows with values &gt; 0.3 to &#39;high&#39; leks_sort&lt;-leks[order(leks$nest_type),] # sort data frame by nest_type Above we also use the order() function to sort the data frame by our new variable. Note that when we sort the data frame we create a new data frame, so the original data remains unmodified. View the sorted data frame to ensure the new variable is correctly assigned. After creating this variable you can use the tapply function with both the management area (area) and the newly created nest_type variable: lek_summary&lt;-aggregate(x=leks_sort$mean_count,by=list(leks_sort$area,leks_sort$nest_type), FUN=mean) lek_summary ## Group.1 Group.2 x ## 1 D high 35.78 ## 2 E high 33.26 ## 3 F high 17.14 ## 4 G high 30.51 ## 5 H high 8.43 ## 6 D low 21.84 ## 7 E low 23.56 ## 8 F low 8.02 ## 9 G low 14.63 ## 10 H low 13.60 The original data came in the form of a time series over multiple years. These summaries were created using the apply() function, which can be used to apply a function across rows (MARGIN=1) or down columns (MARGIN=2). To use these summaries, load the time series data using the readtable function. lek_ts&lt;-read.table(&quot;DATA/wysg_peak_males_ts_sub.csv&quot;,sep=&quot;,&quot;,header=TRUE) head(lek_ts) ## lekid X2008 X2009 X2010 X2011 X2012 X2013 ## 1 B-Blue Mesa 24 16 13 16 13 9 5 ## 2 B-Grass Creek 2 0 17 4 5 12 7 ## 3 B-Hoodoo 3 31 12 11 9 5 2 ## 4 B-Logging Road 24 42 31 10 12 12 ## 5 C-Innes 34 29 19 14 15 12 ## 6 C-Upton-Fairview 11 13 12 10 17 9 # take mean across last 5 rows and add to new column lek_ts$mean_count&lt;-apply(X=lek_ts[,2:7],MARGIN=1,FUN=mean) head(lek_ts) ## lekid X2008 X2009 X2010 X2011 X2012 X2013 mean_count ## 1 B-Blue Mesa 24 16 13 16 13 9 5 12.0 ## 2 B-Grass Creek 2 0 17 4 5 12 7 7.5 ## 3 B-Hoodoo 3 31 12 11 9 5 2 11.7 ## 4 B-Logging Road 24 42 31 10 12 12 21.8 ## 5 C-Innes 34 29 19 14 15 12 20.5 ## 6 C-Upton-Fairview 11 13 12 10 17 9 12.0 "],["data-visualization.html", "Chapter 5 Data Visualization 5.1 Plotting data 5.2 Manipulating plots 5.3 Adding low-level plotting functions 5.4 Facet plots", " Chapter 5 Data Visualization 5.1 Plotting data In addition to statistical analysis and programming capabilities, R produces publishable quality graphics. Base R has the capacity to create excellent graphs; however, the syntax can sometimes feel a little clunky. Most recent graphing examples that you find online will make use of the wildly popular and exceptionally user-friendly package ggplot2. We will use this package throughout this book. There is a excellent book that describes all the details of this package called the R Graphics Cookbook. This chapter will introduce some basic level plotting and options and show you how to modify these options to customize plots. The next chapter will cover Data Exploration (i.e., summarizing and visualizing your data in order to inform your data analysis approach). The first step is to install and load and the ggplot2 package. We will also load a data set containing information on Eastern Fox Snakes (“efs_growth.csv”). This data set investigates eastern fox snake (Pantherophis gloydi) growth rates (“grate”) in two different regions in Ontario (“region”). Reptiles have indeterminate growth (i.e., grow throughout their entire lives), but their growth rates decline as they get older. The dataset also includes measurements on snout-to-vent length (“svl”; a proxy of age). install.packages(&quot;ggplot2&quot;) library(&quot;ggplot2&quot;) efs&lt;-read.csv(&quot;DATA/efs_growth.csv&quot;,sep=&quot;,&quot;,header=TRUE) head(efs) ## Year days ID region sex SVL grate ## 1 2007 333 4415685726 Essex f 1270 -0.353 ## 2 2010 369 4940181800 Essex m 858 1.178 ## 3 2009 730 4962583337 Essex m 1042 0.340 ## 4 2007 357 133637351a Essex f 1070 0.643 ## 5 2007 415 133768254a Essex f 915 0.815 ## 6 2008 711 134713166a Essex m 816 0.853 str(efs) ## &#39;data.frame&#39;: 207 obs. of 7 variables: ## $ Year : int 2007 2010 2009 2007 2007 2008 2008 2007 2009 2008 ... ## $ days : int 333 369 730 357 415 711 378 368 371 354 ... ## $ ID : chr &quot;4415685726&quot; &quot;4940181800&quot; &quot;4962583337&quot; &quot;133637351a&quot; ... ## $ region: chr &quot;Essex&quot; &quot;Essex&quot; &quot;Essex&quot; &quot;Essex&quot; ... ## $ sex : chr &quot;f&quot; &quot;m&quot; &quot;m&quot; &quot;f&quot; ... ## $ SVL : num 1270 858 1042 1070 915 ... ## $ grate : num -0.353 1.178 0.34 0.643 0.815 ... Imagine that we wanted to make a plot that displayed growth rate as a function of their snout-to-vent length (“svl”; a proxy of age). To plot with ggplot, we use the function ggplot(), which creates a coordinate system that we can then add layers to. The first argument for the ggplot() function is the data set that we want to use. In our case, the data set we want to use is ‘efs’, which we loaded into our environment above. We then add one or more layers to our ggplot with a geom_ function. Geoms are geometric objects, like points, lines, or bars that we can add to our ggplot coordinate system. In this example, we will create a scatterplot with geom_point comparing “svl” to “grate”. ggplot(data=efs,aes(x=SVL, y=grate)) + geom_point() As expected, we can see the negative relationship between snout-to-vent length (i.e, age) and growth. This helps us visualize the data, but we can improve the plot by adding better labels and making other manipulations to better visualize our data. 5.2 Manipulating plots We can specify a number of parameters within the geom_ function. For example, we can modify the colour, transparency, shape, or size of the points. Don’t forget that you can also use ?geom_point to understand what else you can do with this code or other geom_ functions. ggplot(data=efs, aes(x= SVL, y=grate)) + geom_point(colour = &quot;blue&quot;, # change the colour of points alpha = 0.50, # change the transparency shape = 20, # change the shape of the point size = 2) # change the size of the point R use numeric codes to represent different plot characters (pch), specified by the shape option in ggplot2. You can find indexes to these characters online or by typing ?pch into the console. R defaults to the variable name. Frequently, these are difficult to decipher outside of your research group and need to be revised for publication and dissemination. ggplot(data=efs, aes(x= SVL, y=grate)) + geom_point()+ labs(x = &quot;Snout-to-Vent Length (mm)&quot;, y = &quot;Growth Rate (mm/day)&quot;) + scale_x_continuous(breaks = seq(0,1500,100))+ scale_y_continuous(breaks = seq(-1,3,0.5)) All aspects of the graph are adjustable. It is often helpful to sketch your ideal plot first by hand and then figure out how plot it using ggplot2. You can also use themes in ggplot2 to change the overall appearance of your plot and control all non-data display. There are many built-in themes and you can review their details online. You can also create your own theme() that you can replicate across analyses so that all your plots have a similar format. Themes are covered in Chapter 9 of the R Graphics Cookbook. A simple example of how to add a theme to your plot and a legend is here: ggplot(data=efs, aes(x= SVL, y=grate, colour=region)) + geom_point() + labs(x = &quot;Snout-to-Vent Length (mm)&quot;, # change x label y = &quot;Growth Rate (mm/day)&quot;, # change y label colour = &quot;Region&quot;) + # colour code points by Region scale_x_continuous(breaks = seq(0,1500,100)) + # specify x-scale scale_y_continuous(breaks = seq(-1,3,0.5)) + # specify y-scale theme_classic() # specify theme 5.3 Adding low-level plotting functions Low-level plotting functions plot on already existing plots. For example suppose we had preformed a regression and wanted to add a regression line to our plot to further visualize the negative relationship between age and growth rate of Eastern fox snakes. We can do this using the geom_smooth function to add the regression line to the plot. ggplot(data=efs, aes(x= SVL, y=grate, colour=region)) + geom_point()+ labs(x = &quot;Snout-to-Vent Length (mm)&quot;, y = &quot;Growth Rate (mm/day)&quot;, colour = &quot;Region&quot;) + scale_x_continuous(breaks = seq(0,1500,100))+ scale_y_continuous(breaks = seq(-1,3,0.5)) + theme_classic()+ # add a theme geom_smooth(method=&quot;lm&quot;) # add a regression line to our plot ## `geom_smooth()` using formula = &#39;y ~ x&#39; 5.4 Facet plots When displaying data it is often useful to have multiple items highlighted on a single plot or to have a multi-panel plot for related data. Below we provide an example using this data to make a facet plot with the facet_grid function. Again make sure you use help to understand what is happening with each line of code if it is not familiar to you (e.g, ?facet_grid). Here, we are going to create a plot that compares growth rate with age for females and males at each different region. We will produce two separate plots for each region that compare males and females. We will also add regression lines. ggplot(data=efs, aes(x= SVL, y=grate, colour=sex)) + geom_point()+ labs(x = &quot;Snout-to-Vent Length (mm)&quot;, y = &quot;Growth Rate (mm/day)&quot;, colour = &quot;Region&quot;) + scale_x_continuous(breaks = seq(0,1500,100)) + scale_y_continuous(breaks = seq(-1,3,0.5)) + theme_classic() + # add a theme geom_smooth(method=&quot;lm&quot;) + # add a regression line to our plot facet_grid(vars(region)) # add the facet grid for a variable of your choice ## `geom_smooth()` using formula = &#39;y ~ x&#39; Developing your plots in R is a great way to improve and maintain your R skills. This may seem like extra work compared to point-and-clicking your way through Excel to create a similar plot; but coding your plots allows for easy replication, update, sharing, and transparency. The code may look fairly complex to create a simple looking plot, but much of the code repeats and can be reused in future efforts. The last thing we will do is save our plot to our working directory. You can do this through RStudio with the Export function in the plotting window; however, it is more useful to learn how to save with code so that you can make specifications to your figure size and file type. This can be accomplished using the ggsave function to output the figure with a specific size and file type. All journals have specifications regarding the size, file type, and resolution of figures for publication. The ggsave function an associated options ensures that you meet those criteria when submitting your research for publication. ggsave (&quot;efs.figure.pdf&quot;, device = &quot;pdf&quot;, width = 7.5, height = 5, units = &quot;in&quot;) "],["data-exploration.html", "Chapter 6 Data Exploration 6.1 Outliers 6.2 Homoscedasticity (homegeneity of variance) 6.3 Distribution of the response variable 6.4 Collinearity among x variables", " Chapter 6 Data Exploration Before conducting any statistical analysis you need to graphically explore your data. There are many packages within R to assist you with the process of data exploration. Many of the reasons behind data exploration and methodology are outlined in a review article in Methods in Ecology and Evolution by Zuur et al. (2010). We are pattern matching machines. We have evolved to visually detect patterns among the chaos. It is important to use that strength to visually inspect your data, understand its nuances, and allow that information to help inform your analyses in model structure. We will cover general (and generalized) linear models in detail later in the course. However, it is necessary to introduce a simple model here to better explain why we recommend generating the plots in this chapter. \\[y_i = \\beta_0 + \\beta_1x_1 + \\beta_2x_2 + ...+ \\beta_jx_i + \\epsilon_i\\] In this model, we are trying to understand the relationship between a number of covariates (\\(x_1, x_2\\)) on the response variable of interest (\\(y_i\\)). The \\(x_i\\) and \\(y_1\\) represent actual data and measurements we have gathered. The model used that information to estimate the relationships among those variables. The shape and direction of the relationship between a particular \\(x_i\\) and our response variable \\(y_i\\) is describe by the slope estimates (e.g., \\(\\beta_1, \\beta_2\\)). In this relatively simple example, the model estimates two other parameters. The model intercept, \\(\\beta_0\\) and the error term \\(\\epsilon_i\\). This type of model can contain multiple \\(x\\) variables and both the \\(y\\) and \\(x\\) values may be discrete (i.e, values are individual, separated and distinct) or continuous (i.e., unbroken values between a range). The nature of the data (i.e., continuous or discrete) will change how you plot the variables. For this example, we will work with sage-grouse lek data. One of the key questions driving much of wildlife ecology (honestly, much of most of ecology) is: how many animals are there? With the obvious follow up question of: why? Animal populations are influenced by a wide variety of environmental feature. Suppose we were interested in determining if mean peak male counts (continuous \\(y\\) variable) were influenced by landscape or climate variables (continuous \\(x\\) variables) within the different management units (i.e., a discrete \\(x\\) variable) or within high and low nesting quality habitats (i.e., another discrete \\(x\\) variable). Before you begin with the code you need to load the lek data and add a grouping factor named “nest_type” which divides the leks into high (&gt;0.3) and low suitability (&lt;=0.3). This was covered in previous chapters. Unfortunately, in almost all field-base research, missing data is a reality. This is true of the sage-grouse data that we are using. Not all leks are surveyed every year. We will discuss how to deal with missing data in more depth in other parts of the course. For now, we want to remove leks from the analysis that have ‘too much’ missing data. We have chosen a 50% missing data as a cutoff for inclusion based on research by Dr. Fedy exploring the impact of missing data and frequency of counts on lek population dynamic estimates. Therefore, we create a new lek dataset for exploration and analysis, by removing any leks where no individuals were counted across the whole time period (i.e., inactive leks) and those with more than 50 percent missing data. We are not going to do any statistical analysis on this dataset for now, but below we use graphical analysis to test a number of the statistical assumptions outlined in Zuur et al. (2010). library(&#39;ggplot2&#39;) leks&lt;-read.csv(&quot;DATA/wysg_peak_males.csv&quot;,sep=&quot;,&quot;,header=TRUE) leks$nest_type&lt;-&quot;low&quot; # create a nesting suitability variable leks[leks$mean_nest&gt;0.3,&quot;nest_type&quot;]&lt;-&quot;high&quot; # change rows with values &gt; 0.3 to &#39;high&#39; leks_sub&lt;-leks[leks$max_count != 0,] # subset with non-zero rows leks_sub&lt;-leks_sub[leks_sub$per_miss &lt; 50,] # subset for missing data 6.1 Outliers Statistical outliers (i.e., data points that are distant from other data points in the dataset) can dominate the results of statistical analysis. Thus, outliers should be identified prior to statistical analysis so they can be removed or fixed if proper justification is found (e.g., error in data entry) or their effects on the analyses can be quantified if they are true biological outliers. As in Zuur et al., 2010 we use box plots and Cleveland dot plots on our y variable. You will see below that although there are no egregious outliers, some may warrant attention. To create the 2-panel plot, we will use the cowplot package. Don’t forget to install and load the package before using the functions that require cowplot. Try and play around a bit with the plot_grid function, see how you can create different plot displays. # To make the two panel plot, we first assign the two plots to objects in R # Create the boxplot boxplot &lt;- ggplot(leks_sub, aes(y=mean_count))+ geom_boxplot() + labs(y=&quot;Mean peak males&quot;) + theme_bw() # Create the Cleveland dot chart dotplot &lt;- ggplot(leks_sub, aes(x=mean_count, y=seq(1, length(mean_count),1))) + geom_point() + labs(x=&quot;Mean peak males&quot;, y = &quot;Order of the data&quot;) + theme_bw() # Now plot them in the same window using the cowplot package #install.packages(&quot;cowplot&quot;, quiet=TRUE) # code for installing package library(&quot;cowplot&quot;, quietly=TRUE) cowplot::plot_grid(boxplot, dotplot) In the previous chapter we created multipanel plots using the facet_grid function. Here we used the cowplot to create a multipanel plot that displays the results from the two separate plots. Below we plot the mean peak male count and a number of landscape and climatic factors that are potentially having an effect on the number of males in a single plot. Before creating this plot, we create a subset matrix that can be used with the facet_wrap function. Once we create the new susbet matrix of data that only contains the mean_count and climate variables of interest, we will use code formatted based on the ‘tidyverse’, which uses pipes (%&gt;%), to format the dataset prior to plotting. The ‘tidyverse’ is a popular data science group of packages and coding style used to ‘tidy’ messy data to facilitate analysis and visualization. I use ‘tidyverse’ occasionally, but I am not a diehard convert. If you want to delve deeper into the ‘tidyverse’ I recommend the excellent book, R for Data Science which delves deep into the tidyverse universe and covers many important topics in data science. I take a pragmatic approach throughout this book and the course. I do not really care what packages you use, as long as it works. library(&quot;tidyverse&quot;, quietly=TRUE) # create a matrix by binding only the columns we are interested in ex_data&lt;-cbind(leks_sub$mean_count,leks_sub$length_road,leks_sub$mean_sb, leks_sub$mean_herb,leks_sub$mean_nest,leks_sub$mean_ppt) # name the columns colnames(ex_data)&lt;-c(&quot;mean_count&quot;,&quot;length_road&quot;,&quot;mean_sb&quot;, &quot;mean_herb&quot;,&quot;mean_nest&quot;,&quot;mean_ppt&quot;) # create a new data set by converting the matrix to a data frame ex_data2 &lt;- ex_data %&gt;% as.data.frame() %&gt;% gather(key = &quot;variable&quot;, value = &quot;value&quot;) # plot the data ggplot(ex_data2, aes(x=value, y=seq(1, length(value),1))) + geom_point() + labs(x=&quot;Value of the variable&quot;, y = &quot;Order of the data&quot;) + theme_bw() + facet_wrap(~variable, scales = &quot;free&quot;) 6.2 Homoscedasticity (homegeneity of variance) An important assumption in many statistical analysis is that variance (i.e., spread of data points) is similar between groups (e.g., sexes, different experimental treatments). Essentially, this assumption means that the residuals from a model have equal variance (homoscedasticity) for every fitted value and the predictors. Meeting this assumption ensures accurate calculation of standard errors for the parameter estimates. Thus, if we were interested in determining if the mean peak number of males for leks was different depending on the management area or the surrounding categorical nesting quality it would be important to explore this assumption. Here we see the true value of these graphing packages as they easily allow us to compare the distributions within one or more grouping factors using a box plot. ggplot(leks_sub, aes(x=man_area, y=mean_count))+ geom_boxplot()+ labs(x=&quot;Management Area&quot;, y=&quot;Mean peak males&quot;)+ theme_bw()+ facet_wrap(~nest_type) Based on the plots above, it appears that we are violating the assumption of homogeneity of variance with some management areas having much lower variation in mean peak counts than others. We will discuss how to deal with this type of situation later in the course. But, these results suggest we may need to refine our research questions or account for these differences in our model development. Homoscedacity assumption determines how many variances are to be estimated: either one overall variance of the response variable, or several variances for different values of the explanatory variables. In the latter case, both the mean and the variance of the response variable change with the explanatory variables. We will get deeper into this when we discuss General(ized) Linear Models in later chapters particularly when we are discussing model fit. 6.3 Distribution of the response variable An assumption in many statistical analyses is that your y variable is normally distributed. A simple way to assess this assumption is to visualize your data using a histogram. Here we use a two panel plot to plot both the mean peak number of males and the log(peak number of males), which is a common statistical transformation used to closer approximate a normal distribution. hist_mean &lt;- ggplot(leks_sub, aes(x=mean_count)) + geom_histogram(bins=30) + labs(x=&quot;Mean number of males&quot;, y=&quot;Frequency&quot;)+ theme_bw() hist_log &lt;- ggplot(leks_sub, aes(x=log(mean_count))) + geom_histogram(bins=30) + labs(x=&quot;Log(Mean number of males)&quot;, y=&quot;Frequency&quot;)+ theme_bw() cowplot::plot_grid(hist_mean, hist_log, nrow=2) Again the lattice package can also be useful for plotting multi-panel plots with multiple variables. Here we use the grouping factor (nest_type) to look for normality within each nest group at the same time. ggplot(leks_sub, aes(x=mean_count))+ geom_histogram(bins=30)+ labs(x=&quot;Mean number of males&quot;, y=&quot;Frequency&quot;)+ facet_wrap(~nest_type)+ theme_bw() 6.4 Collinearity among x variables When x variables (i.e., predictors) are highly correlated it is difficult to determine their independent effect on the y variable. Thus, it is always a good idea to determine the level of correlation among your predictors. Here we use a scatter plot to visualize the collinearity between the three landscape variables in the ex_data dataframe. Here will use the ggscatmat function from the GGally package to visualize the correlation between variables. You will also notice in the plot that there is a strong positive correlation between sagebrush habitat and nesting quality, which is not surprising given the ecology of sage-grouse and their association with this habitat type. #install.packages(&quot;GGally&quot;, dependencies=TRUE, quiet=TRUE) library(&quot;GGally&quot;, quietly=TRUE) ## Registered S3 method overwritten by &#39;GGally&#39;: ## method from ## +.gg ggplot2 ggscatmat(ex_data, columns = c(3,4,5)) #columns function selects specific comparisons "],["correlation-and-linear-regression.html", "Chapter 7 Correlation and Linear Regression 7.1 Correlation 7.2 Regression 7.3 Model Validation", " Chapter 7 Correlation and Linear Regression 7.1 Correlation 7.1.1 Correlation coefficient We addressed correlation with some code and examples provided in the Data Exploration chapter. This section will provide a few more details on correlation estimation and how to explore correlations with non-parametric data. Correlation is the most basic means of measuring the association between two variables. Pearson’s correlation (r) is used to estimate the level of correlation among two continuous and normally-distributed variables and ranges from -1 to 1. Perhaps too obvious for comment, but values &lt; 0 indicated a negative relationship, values &gt; 0 indicate a positive relationship with 0 indicating no relationship. There is no test of significance per se for the Pearson. However, there are some general guidelines. Generally speaking, r &lt; |0.3| is considered weak, moderate values are |0.3| &lt; r &lt; |0.7|, and an r &gt; |0.7| is considered high correlation. The “|” around the numbers indicate they can take on a negative or positive value. 7.1.2 Non-parametric correlations Often times data are not normally distributed or continuous. Luckily, a series of other statistics have been developed to measure the relationships among variable with different measurement scales. After Pearson’s correlation coefficient, the next most common approach in ecology and environment is likely the Spearman’s rank correlation. Spearman’s correlates two variables measured on the ordinal scale. The raw data may have actually been measured on an ordinal scale or if data are continuous, but not normally distributed, the researcher may decide to convert the variable to the ordinal scale and assess correlation using a Spearman’s rank correlation. This type of transformation from a non-normal continuous variable to a rank-ordered variable is a common approach in non-parametric statistics (e.g., Wilcoxon, Mann-Whitney U-test) and we encounter it again when dealing with GLMs. An alternative to Spearman’s that you may encounter is Kendall’s tau, which is an alternative test of association between oridinal data. My experience suggests it is less common in the ecology and environment literature. Categorical data provide a different challenge and associations (i.e., correlations) are commonly assessed using the Pearson Chi-squared test for independence. With this type of data we are testing an explicit hypothesis. The null hypothesis (\\(H_0\\)): no association between the two variables, is compared against the alternative hypothesis (\\(H_A\\)): there is an association between the two variables. The Chi-squared test results in a significance test which can reject the null hypothesis. Cramer’s V is an alternative to the Chi-squared test. Cramer’s V does not reject or fail to reject a hypothesis and is more similar to the Pearson’s correlation coefficient in its interpretation producing a value between 0 (no association) and 1 (perfect association). Guidelines suggest that a Cramer’s V of &lt; 0.25 indicates a week association and &gt; 0.75 is a moderate association. All the above approaches assume that individual observations are drawn randomly from the population. 7.2 Regression The simplest linear models we can develop are linear regression models. These models are similar to the concept of correlation introduced in the previous section in that we are only examining two variables. However, linear regression differs from correlation in several important ways. Conceptually, the biggest difference, is that in a linear regression you are proposing a directional relationship. When we looked at correlations, we were simply interested in whether there was a relationship between two variables, but we did not propose a cause and effect relationship. In other words, in correlation, it does not matter what variable is on the \\(y\\) axis and what variable was on the \\(x\\) axis. The variables are interchangeable between axes. This is not true of a linear regression. For a linear regression, we must specify which of the variables we are trying to predict (i.e., which variable is the dependent variable. This variable is the \\(y\\) variable. The predictor (i.e., independent variable) is the \\(x\\) variable. Our selection of dependent \\(y\\) and independent \\(x\\) variables is informed by our question and the hypotheses that we want to test. Essentially, the \\(y\\) is what you want to predict - your response variable of interest - and your \\(x\\) is something that influences that response. We will go through many examples throughout the book. These models form the basis for all subsequent and more sophisticated modelling that we will explore in this book. Essentially, a simple linear regression models a linear (i.e., straight line) relationship between a response variable \\(y\\) with a normal distribution and a single, numerical, explanatory variable. We introduced the linear model in the past chapter. A simple linear regression is even… well… simpler and can be represented by the following equation. \\[y_i = \\beta_0 + \\beta_1x_1 + \\epsilon_i\\] Another common way of representing this equation involves presenting more detail about the shape of the response variable. In this case, we specify the response variable has a normal distribution (\\(Y\\) ~ Normal(\\(\\mu_Y, \\sigma^2_Y\\))) and therefore our model is written as: \\[\\mu_Y = \\beta_0 + \\beta_1x_1 + \\epsilon_i\\] The \\(\\beta_0\\) and \\(\\beta_1x_1\\) have the same interpretation. As a reminder, \\(\\beta_0\\) is the intercept (i.e., where the line crosses the \\(y\\) axis). Or, in other words, \\(\\beta_0\\) is the predicted value of \\(\\mu_y\\) when \\(x\\) = 0. The slope of the straight line is \\(\\beta_1\\). I once had a highly respected and intelligent professor tell me that, in ecology, “it is all about the Betas”. This slope can be presented as the rate of change in \\(\\mu_Y\\) per unit change in the explanatory variable \\(x_1\\). Throughout the book, I have tried to use ‘real’ ecological data collected by me and my colleagues. I think this is important because the data are messy and frequently do not behave well. These data can have challenging distributions that can make fitting models to the data difficult. Sometimes wrangling ‘real’ world ecological data and fitting models to those data is as frustrating and challenging as stuffing a tired and angry toddler into a snow suit… in a hot room… while sleep deprived… with your mother-in-law explaining how you are going to be late while she muses that she never had such problems with her kids. But, working with those data will help you with your own data. However, while digging through all the many datasets I have compiled across my career, it turns out I have never collected data that could be modeled using a simple linear regression. Lucky me. So the following data and examples are from the excellent textbook by Pablo Inchausti, Statistical Modeling with R, which is currently available through the UW library for this course. The author’s website with data, code, and extra material can be found here. The data we will work with look at plant tolerance and cadmium levels. Cadmium (Cd) is a metal that has toxic effects on plant and human health. The researchers grew grass in soils with different concentrations of Cd to determine if the grass was a bioaccumulator of the metal to assess its usefulness in bioremediation. More details can be found in the textbook. First, we will make sure the packages required are loaded. Remember, you must make sure they are installed. packages.needed &lt;- c(&quot;ggplot2&quot;, &quot;broom&quot;, &quot;qqplotr&quot;, &quot;cowplot&quot;, &quot;ggeffects&quot;) lapply(packages.needed, FUN = require, character.only = TRUE) Next we load the data provided by Inchausti. The data can be downloaded from the author’s website (referenced above) or the UW LEARN site. df = read.csv(&quot;DATA/Ch 04 Cadmium.csv&quot;, header = TRUE) summary (df) ## soil shoot root ## Min. : 60 Min. : 16.2 Min. : 105 ## 1st Qu.:120 1st Qu.: 52.6 1st Qu.: 245 ## Median :180 Median :123.7 Median : 465 ## Mean :180 Mean :117.0 Mean : 503 ## 3rd Qu.:240 3rd Qu.:171.2 3rd Qu.: 665 ## Max. :300 Max. :217.7 Max. :1067 The soil variable represents the level of Cd in the soil and the shoot variable represents the amount of Cd in the shoots. The model we want to fit is: \\[shoot = \\beta_0 + \\beta_1soil + \\epsilon_1\\] The \\(\\epsilon\\) represents the residuals and there are explicit assumptions about its distribution also. In this case we assume the residuals will follow a normal distribution with a mean = 0. Fitting a simple linear regression in R uses the lm function which is part of the stats package. You do not need to load this package because it is automatically loaded by default at the start of every R session. We fit the linear model to our data using the following: m1 &lt;- lm(shoot ~ soil, data = df) The code above fits the model we specified in the equation above. The shoot response variable and soil predictor covariate are present in the code. However, we do not need to specify any of the model-estimated components such as the intercept \\(\\beta_0\\) the slope of the relationship \\(\\beta_1\\) or the error term \\(\\epsilon_1\\) because these are unknown to us and are produced by the model estimating function lm. The tilde sign ~ in R fills the place of the = in the equation and separates the response and predictor variables. We also must specify the data frame which contains the variables with the data = df code. We store the results by assigning the model to an object (m1 in the example above) which creates a list containing model results. The most relevant model results are provided using the summary command. summary(m1) ## ## Call: ## lm(formula = shoot ~ soil, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -25.97 -11.22 1.73 7.66 28.68 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -19.030 8.355 -2.28 0.035 * ## soil 0.756 0.042 18.00 5.9e-13 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 15.9 on 18 degrees of freedom ## Multiple R-squared: 0.947, Adjusted R-squared: 0.944 ## F-statistic: 324 on 1 and 18 DF, p-value: 5.88e-13 7.2.1 Interpreting Output The most important model estimates are presented under the Coefficients heading (remember “It’s all about the Betas”). The summary output presents estimates of the intercept (\\(\\beta_0\\)), the slope (\\(\\beta_1soil\\)), and the standard errors associated with each estimate (i.e., the variation around the estimate). The Coefficients section also presents the t value and the p-value (Pr(&gt;|t|)). Recall from lecture, that within this frequentist paradigm all of these parameters are estimated using a maximum likelihood approach. The p-value is calculated based on the t-distribution which is a type of normal distribution, centered on zero, in which the variance is estimated based on the degrees of freedom. The degrees of freedom are related to your sample size. Increasing the sample size decreases the variance, essentially narrowing your normal distribution. The p-value presented in the output represents the probability of obtaining a t-value equal to or more extreme than the one obtained if the null hypothesis is true (i.e. \\(\\beta_1\\) = 0). Therefore, the p-value essentially measures the lack of congruence between the data and the null hypothesis. In a frequentist context, the p-value essentially answers the question of how frequently we would observe the data if we ran this experiment many times. Data that deviate from the null hypothesis, based on model assumptions, have lower p-values. We will work through a simulation exercise in class that will hopefully be illuminative. In order to determine whether the results are actually important we must interpret the magnitude of the estimated slope and its error in the context of the ecology and research questions that motivated the data collection. In the example above, that means we need to decide if the absorption of 75.5% of the soil Cd by plants is important. The estimated intercept \\(\\beta_0\\) of -19.03 is largely meaningless in the context of this model because it estimates the average shoot concentration when there is no Cd in the soil (i.e. Cd = 0). Later we will discuss the possibility of standardizing your predictor variables and how that affects our interpretation of the intercept in linear models. The bottom of the output presents several measures of the model performance. Each model can be used to generate predicted values (i.e., the value of the response variable if the model is true). The Residual standar error essentially represents difference between observed and predicted values (i.e, the estimated variance of the residuals or unexplained variance in the model). The Multiple R-squared and the Adjusted R-squared values are the proportion of variation in the response variable that is explained by the model. You can find plenty of mathematical descriptions of the difference between these two metrics online, if you are interested. The main difference is that Multiple R-squared will stay the same or increase as you add new predictors (i.e., covariates) to the model. This is true even if the new predictors are uninformative. Whereas Adjusted R-squared only increases if the new predictors improve the models predictive power and will decrease if the predictors are irrelevant to the models predictive performance. These values are both a measure of the goodness of fit of the model to the data. Higher values represent a better fit to the data, but should largely only be considered in a comparative context (i.e., in relation to other models fitted to the same data). The Residuals: section at the top of the model output is largely irrelevant to model interpretation. Statistics is so relevant to the real world because it allows us to estimate the inherent uncertainty underlying any complex problem. The Std. Error component give us some indication of the variability around the estimated parameter, but it is typically considered a fairly narrow or overly optimistic estimate of our confidence in the estimate. Therefore, it is common to estimate the confidence interval surrounding the parameter estimate. The interpretation of a confidence interval in a frequentist context is different than in a Bayesian context. In a frequentist context, the confidence interval is a range of equally plausible values of the parameter that we could obtain if we repeated this study many times with the same sample size and the same population. As an aside, in a Bayesian context we would estimate a 95% credible interval which would be accurately interpreted as: there is a 95% chance the true value is within the range presented. This is different. To obtain the confidence interval for the model m1 we use the confint( ) function. confint(m1) ## 2.5 % 97.5 % ## (Intercept) -36.584 -1.476 ## soil 0.668 0.844 So, what is the best way to interpret confidence intervals in the frequentist context? Consider it as a thought experiment. If we ran this study over and over again in the same population with the same sample size, we would end up with a distribution of parameter estimates with a lower 2.5% bound of 0.668 and an upper 97.5% bound of 0.844. Hopefully the example in class helps to further clarify this concept. 7.3 Model Validation It is not enough to fit a model and simply present the parameter estimates. We also need to determine if it is a ‘good’ model. In other words, does the model fit the data well? The residuals and their distribution are the main focus of model validation. Essentially, the smaller the residuals (i.e., the difference between the observed and predicted values) the better the model fits the data. In addition to the basics of minimizing \\(Y_{obs} - Y_{pred}\\) we also want simple and random patterns in the distribution of our residuals. Any trends in the residuals can be an indication of poor model fit. With a general linear model, the default model validation summary produces four graphs. You can produce these in base R with the plot(model_name) function. However, for greater flexibility and insight we will produce validation figures using ggplot. This approach is also used throughout most of the examples in Inchausti’s textbook. The first graph plots the Residuals vs. the Fitted values. The residuals are estimated for each of the observed values and are presented in the y-axis. Remember the residuals are estimated as \\(Y_{obs} - Y_{pred}\\) and can therefore take on positive and negative values that should be distributed randomly around 0. The fitted values represent the model predicted values,encompass the range of your observed data, and are plotted on the x-axis. If the model fits the data well, we should see an even distribution of points scattered on both sides of the 0 value. There should be no visible pattern to the distributed values (i.e., cone shaped). This implies a homogeneous distribution of the residuals, which is an informal assessment of homoscedacity (i.e., homogeneity of variance). A parallel plot to the Residuals vs. Fitted values, replaces the x-axis fitted values with the actual observed values and should have a similar pattern. The third common plot presents the sample quantiles on the y-axis and the theoretical quantiles on the x-axis. Since this plot presents the two quantiles and is used to assess the ‘normality’ of the data, it is often refereed to as the “Normal Q-Q plot”. Quantiles are points in your data below which a certain proportion of your data fall. In a normal distribution with a mean of 0, the 0.5 quantile (i.e., 50th percentile) is 0. The quantiles presented are basically your data sorted in ascending order with each data point labelled as the point below which a certain proportion of the data fall. Therefore, if the model predicted (Theoretical quantiles) match the observed (Sample quantiles) perfectly, they will lie along a perfect diagonal. Thus,if the residuals are normally distributed, they should have a relatively tight scatter of points along the line of perfect fit. It is very common for the points near the tail ends of the distribution (upper and lower) to deviate more from the theoretical perfect fit than the points closer to the center for the distribution. This makes sense because there are more data to inform model fit in the central regions of the distribution than near the tails. The final common model validation plot presents Cook’s distance. This is essentially a sensitivity analysis that quantifies the impact of each data point on the parameter estimates. Data points with a “large” Cook’s distance may have an unduly large influence on the parameter estimates. There is no hard-and-fast rule to decide when a Cook’s distance is “too” large. Like much of model validation, it is somewhat subjective and relies on our visual interpretation of the validation plots. If you are concerned about the influence of a particular observation, you can remove that observation, re-run the model and look at the changes in the model parameter estimates. The Cook’s distance plot presents the observations in your data set in order along the x-axis so it is easy to identify potentially large observations. The y-axis presents the Cook’s distance. If you are interested, I will leave you to look up the actual calculation of Cook’s distance on your own. It is not necessary to understand the underlying mathematics in detail. It is enough to know what it represents and how it can inform your modelling choices and interpretation. Producing ggplot-type figures requires use of the package broom. First, we create a tibble. tidy(m1, conf.int=TRUE) ## # A tibble: 2 × 7 ## term estimate std.error statistic p.value conf.low conf.high ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -19.0 8.36 -2.28 3.52e- 2 -36.6 -1.48 ## 2 soil 0.756 0.0420 18.0 5.88e-13 0.668 0.844 The glance function gathers the model fit data into a more user-friendly format. glance(m1) ## # A tibble: 1 × 12 ## r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.947 0.944 15.9 324. 5.88e-13 1 -82.7 171. 174. ## # ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt; Then we create a data frame containing all the information required to produce the four plots. Most importantly, this data frame includes the empirical data, the model fitted values, Cook’s distance, residuals. You can follow the ggplot code below to see what variables are used to create each plot. The code provided comes directly from Inchausti Ch.4 with some slight modifications. The augment function accepts a model object and adds information about each observation in the dataset. These are the values we will plot. augment(m1) ## # A tibble: 20 × 8 ## shoot soil .fitted .resid .hat .sigma .cooksd .std.resid ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 23.2 60 26.3 -3.12 0.150 16.4 0.00398 -0.212 ## 2 16.2 60 26.3 -10.1 0.15 16.2 0.0419 -0.689 ## 3 52.7 60 26.3 26.4 0.15 14.9 0.285 1.80 ## 4 29.1 60 26.3 2.78 0.15 16.4 0.00316 0.189 ## 5 52.5 120 71.7 -19.2 0.075 15.7 0.0634 -1.25 ## 6 45.7 120 71.7 -26.0 0.075 15.0 0.116 -1.69 ## 7 52.9 120 71.7 -18.8 0.075 15.7 0.0608 -1.22 ... Notice the table includes one row for each of the observations (only display 7 above). The next step is to store the results of augment(m1) in order to create the plots. res.m1 &lt;- augment(m1) # ggplots of the residual analysis of of Frequentist simple regression m1.res=ggplot(data=res.m1, aes(x=.fitted, y=.resid)) + geom_point(size=1) + theme_bw() + geom_hline(yintercept = 0) + labs(x=&quot;Fitted values&quot;, y=&quot;Standard residuals&quot;) + theme(axis.title=element_text(size=12), axis.text = element_text(size=10)) m1.res.vs.expl=ggplot(data=res.m1, aes(x=soil, y=.resid)) + geom_point(size=1) + theme_bw() + geom_hline(yintercept = 0) + labs(x=&quot;Cd in soil&quot;, y=&quot;stnd residuals&quot;) + theme(axis.title.x=element_text(size=12), axis.title.y = element_blank(), axis.text = element_text(size=10)) m1.Cook=ggplot(data=res.m1, aes(x=1:nrow(res.m1),y=.cooksd)) + geom_linerange(aes(ymin=0, ymax=.cooksd)) + theme_bw() + labs(x=&quot;Data point&quot;, y=&quot;Cook distance&quot;)+ theme(axis.title=element_text(size=12), axis.text = element_text(size=10)) m1.qq=ggplot(res.m1, mapping=aes(sample = .std.resid)) + stat_qq_point() + theme_bw() + stat_qq_line() + stat_qq_band(alpha=0.3) + labs(x = &quot;Theoretical quantiles&quot;, y = &quot;Sample quantiles&quot;) + theme(axis.title=element_text(size=12), axis.text = element_text(size=10)) plot_grid(m1.res, m1.res.vs.expl, m1.qq, m1.Cook, ncol=2,labels = LETTERS[1:4], align=&quot;hv&quot;,label_x=0.85, label_y=0.95) # Fig 4.5 in Inchausti "],["logistic-regression.html", "Chapter 8 Logistic Regression 8.1 Binary Response Variable 8.2 Ecological Applications 8.3 Loggerhead Shrike Example", " Chapter 8 Logistic Regression 8.1 Binary Response Variable As with the previous chapter, much of the material presented here has been adapted from Inchausti. However, I have added a large amount of information that makes the content more relevant to the course content and student’s research projects. Logistic regression is used when the response variable takes on only two values. In these cases the binomial distribution is the only possible probability distribution. It was developed to calculate the probability of obtaining a “success”. In most cases in ecology and environment the definition of “success” is somewhat arbitrary and can take on many different ecological definitions. Originally success was defined as the ‘desired’ outcome of a binary event. However, we try to ‘desire’ any particular outcome. In logistic regression the response variable are coded as either 0 or 1. In other words, the event you are trying to predict either occurred or it did not occur. As with other GLMs, the objective of a logistic regression is to understand the relationship between the explanatory (\\(x\\)) variables and the mean of the response variable. However, as explained in lecture, when the response variable is not normally distributed, this relationship requires the link function (\\(g\\)). The link function is applied to the response variable in order to ‘linearize’ the relationship. The general role of the link function is represented by the following equation: \\[g(E(Y)) = X\\beta\\] Generally, this equation means that the product of the link function, \\(g\\) and the expected value of \\(Y\\) \\(E(Y))\\) is explained by the explanatory covariates \\(X\\) and the estimated coefficients \\(\\beta\\). The logit link function is used for logistic regression. 8.1.1 Logit Link Function Typically, the link function is the most challenging aspect of GLMs for new students to understand. In order to understand the logit link function and how it applies to logistic regression, we need to first understand the fairly intuitive concept of the odds ratio. Given that our outcome variable is a 1 or 0, we are assuming the events are independent of each other. We frequently relax this assumption in some applications of logistic regression in ecology, but we will ignore that complexity for now and focus on events that are truly independent of each other. That means if the probability of an event (often indicated by Greek letter \\(\\pi\\)) occurring is 0.75 we know the probability of the even not occurring is 0.25 or (1 - \\(\\pi\\)) = 0.25. Therefore (and this is important) we can represent the relative likelihood of two events (1 or 0) as the ratio of their probabilities: \\[\\frac{\\pi}{(1-\\pi)} = \\frac{0.75}{0.25} = 3:1\\] This is the odds ratio. We interpret this to mean that the occurrence of the event (1) is three times more likely than the other condition (e.g., non-occurrence or 0). Usually, statistics text books and examples use coin flips. The odds ratio of a fair coin flip is 0.5 or 1:1. The odds ratio maps the probability \\(\\pi\\) of an event onto the positive real line [0, $$]. The logit link function is the log of an odds ratio: \\[logit(\\pi) = log\\frac{\\pi}{(1-\\pi)}\\] This function maps the odds ratio onto the real line (i.e., a line with a fixed scale such that every real number corresponds to a unique point on the line). This, in effect, ‘linearizes’ the relationship between our explanatory \\(x\\) variables and our binary response variable. Therefore, the logistic regression establishes a linear relation between the mean of the response variable \\(E(Y)\\) and the explanatory variables as logit(\\(\\pi\\)) = \\(X\\beta\\). The response variable [1, 0] is not transformed in anyway. It is now related to the explanatory variables using the logit link function. Once the parameters \\(\\beta\\) are estimated, the equation includes the logit link on the left hand side of the equation and the relationship between \\(\\pi\\) and \\(X\\) is then linear on the scale of the link function. In order to compare the observed outcome data (1, 0) with model predictions, one more step is required. The link function must be inverted to obtain the predictive relationship between each side of the equation. This is necessary because the logit function maps a probability onto the real line [0, \\(\\infty\\)], but the inverse maps any value of \\(X\\) onto the the unit interval [0,1] of interest to predict the \\(\\pi\\) parameter of the binomial distribution (along with mean and variance) as a function of the explanatory variables. The inverse logit function is: \\[logit^{-1}(\\pi) = \\frac{exp(X\\beta)}{1+exp(X\\beta)}\\] This inverse logit function takes the values of the explanatory variables \\(X\\) and the associated estimated \\(\\beta\\) parameters and predicts the probability of observing a ‘success’ (i.e., 1) for every value of the explanatory variables. From a practical standpoint, the link function allows us to produce continuous predictions across each value of \\(X\\) to understand how those variable influence the probability of a “success” or positive outcome (i.e., 1) 8.2 Ecological Applications One of the most common application of logistic regression in ecology is to predict a species presence. In order to build these types of models it is critical that we have data (i.e., explanatory variables) that characterize where the species is present [1] and where the species is absent [0]. If we only have presence data, then a logistic regression is not appropriate because, functionally, all we have are 1s in our response variable. We know the presence of animals and plants is almost always non-random and can be influenced by many potential covariates including both biotic and abiotic explanatory variables. Often times these explanatory data are collected in the field by research technicians; however, it is also common to use spatial data (e.g., GIS data) to summarize relevant ecological covariates. 8.3 Loggerhead Shrike Example Details on the study (e.g., questions, methods) will be provided in lecture. Here we will work with the data and fit a logistic regression. #Load Packages--- list.of.packages &lt;- c( &quot;ggplot2&quot;, # for graphing &quot;ggeffects&quot;, # model effects plots &quot;GGally&quot;, # ggpairs &quot;broom&quot;, # for plotting coefficients &quot;DHARMa&quot;, # Residual analysis &quot;qqplotr&quot;, # residual plots &quot;cowplot&quot;) # plot_grid # Check you have them in your library new.packages &lt;- list.of.packages[!(list.of.packages %in% installed.packages()[,&quot;Package&quot;])] # install them if you don&#39;t have them and then load. if(length(new.packages)) install.packages(new.packages,repos = &quot;http://cran.us.r-project.org&quot;) lapply(list.of.packages, require, character.only = TRUE) Next we load the data. The data can be downloaded from the UW LEARN site. LOSHdata&lt;-read.csv(&quot;DATA/LOSHdataFinal.csv&quot;,header=TRUE, as.is=TRUE) # case needs to be a factor for ggpredict to work LOSHdata$case &lt;- as.factor(LOSHdata$case) summary (LOSHdata) # examine the data - not printed here 8.3.1 Standardize variables Recall from previous lectures the importance of standardizing variables prior to modelling. Here we use a custom user-defined function for standardization for clarity and to present an example of a simple function. However, this can also be achieved with the dplyr package using mutate and scale functions. ##standardizing the variables - putting them on the same scale standfunc&lt;-function(x){(x-mean(x,na.rm=T))/sd(x,na.rm=T)} # create a function to standardize the values (center the mean on 0) colnames(LOSHdata) LOSHdata$StSageHgtSD&lt;-standfunc(LOSHdata$SageHgtSD) LOSHdata$StBare&lt;-standfunc(LOSHdata$Bare) LOSHdata$StHerb&lt;-standfunc(LOSHdata$Herb) LOSHdata$StSage&lt;-standfunc(LOSHdata$Sage) LOSHdata$StShrub&lt;-standfunc(LOSHdata$Shrub) LOSHdata$StRough200m&lt;-standfunc(LOSHdata$Rough200m) LOSHdata$StFenceDist&lt;-standfunc(LOSHdata$FenceDist) LOSHdata$StPowerDist&lt;-standfunc(LOSHdata$PowerDist) 8.3.2 Correlations Examine correlations in potential explanatory variables. ggpairs(data = LOSHdata, columns = 10:16, mapping = aes(color=factor(case), shape=factor(case)), upper = list(continuous = wrap(&quot;cor&quot;, size=2.5))) + theme_bw() + theme(strip.background = element_rect(colour=&quot;black&quot;, fill=&quot;white&quot;), strip.text=element_text(size=6), axis.text = element_text(size=6, angle=30, hjust=0.5)) As a reminder, when potential explanatory variables are highly correlated, they cannot be included in the same model. Therefore, if two variables are highly correlated you have to choose one of them to retain in the final model statement. We went through multiple approaches for making this decision in class. As you can see in the correlation plot above, we have a number of highly correlated variables. These correlations are not particularly surprising given what we know about the sagebrush ecosystem. For example, there is high positive correlation (r = 0.97) between sagebrush cover and shrub cover which was expected given that sagebrush (Artemisia sp) was the dominant shrub species in the study site. Additionally, we see a strong negative correlation (r = -0.81) between bare ground and herbaceous cover. 8.3.3 Model Fitting The syntax for fitting a logistic regression in R is very similar to the syntax we have seen so far and uses the glm function. This function is in the stats package which is loaded by default every time you initiate R. Of the seven candidate variables remaining we excluded Bare and Shrub due to high correlations. We chose to drop those variables because they are less relevant to ecological questions of interest and management activities within sagebrush ecosystems. When using the glm function you must specify the probability distribution used to model the response variable. For logistic regression, this is typically family = binomial which has the default logit link function. Therefore, our model including all remaining covariates is written in R as: m1 &lt;- glm(case ~ StSageHgtSD + StHerb + StSage + StRough200m + StFenceDist, family = binomial, data = LOSHdata) summary(m1) ## ## Call: ## glm(formula = case ~ StSageHgtSD + StHerb + StSage + StRough200m + ## StFenceDist, family = binomial, data = LOSHdata) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -3.431 0.335 -10.24 &lt;2e-16 *** ## StSageHgtSD -0.602 0.362 -1.66 0.097 . ## StHerb -0.781 0.415 -1.88 0.060 . ## StSage 0.184 0.299 0.61 0.539 ## StRough200m 0.078 0.266 0.29 0.770 ## StFenceDist -0.759 0.352 -2.15 0.031 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 160.81 on 419 degrees of freedom ## Residual deviance: 147.45 on 414 degrees of freedom ## AIC: 159.4 ## ## Number of Fisher Scoring iterations: 7 The model output is very similar to that of the general linear model explored earlier in the course. The output includes the model intercept, the partial slopes for each variable included in the model, the standard error associated with each estimate, the Wald statistic (= \\(\\beta\\) estimate/SE(estimate)), and the statistical significance indicating whether the coefficient estimate differs ‘significantly’ from zero. The Null deviance is the estimated deviance of an intercept-only model and the Residual deviance is the remaining deviance after including the five explanatory variables. The greater the reduction from the null to the residual deviance, the greater the proportion of deviance explained by the model. We can calculate McFadden’s analogue of R^2 which indicates the reduction from null to the residual deviance as: 1 - (m1$deviance / m1$null.deviance) ## [1] 0.0831 The AIC value is used to compare models developed with the same data. Since we standardized the variables so they have a mean of zero, we can extract some information from the intercept. The intercept of -3.43 is the value of logit(\\(\\pi\\)) for a “typical” site with average values of all the explanatory variables. We can transform this value into an odds ratio by ‘exponentiating’ (is that a word?) the intercept value exp(-3.43) = 0.03. This value is much smaller than one, indicating LOSH nests are much more likely to be absent then present in a typical site. There are some assumptions that we have made in our model, which may not be true. We will discuss these assumptions and their potential impact in class. The main assumptions are associated with ‘true’ vs. ‘psuedo’ absences and sample size in our unused sites. Scaling variables prior to modelling has the added benefit of facilitating direct comparison of variables measured across different scales. We can compare the magnitude of the \\(\\beta\\) coefficient estimates directly. We can compare the relative ‘strength’ of an effect with simple division. Two of the most influential variables are herbaceous cover and distance to a fence. We can see the estimates are close to each other, but the herbaceous cover estimate is slightly higher. Dividing one into the other (-0.7814/-0.7589 = 1.03) tells us the relative effect of herbaceous cover is only 3% stronger than the relative effect of distance to fence. Recall that the \\(\\beta\\) coefficient for standardized variable indicates the changes in logit(\\(\\pi\\)) with a change in one standard deviation of an explanatory variable. In order to interpret this on actual scale of our data, we need to calculate the standard deviation of the explanatory variables of interest. As an example, we will focus on distance to fence. However, there is an important consideration when interpreting the coefficients on distance variables. In habitat selection studies, such as this one, a positive coefficient estimate typically denotes a preference for a particular habitat component. In other words, an increasing value in the \\(x\\) (e.g. a particular habitat component) indicates an increasing probability of use or selection and the opposite is true in that a negative coefficient estimate indicates avoidance of a particular habitat type. However, distances are unique. If you are using the Euclidean distance to a feature, a negative coefficient estimate indicates a preference for that habitat type. In other word, the probability of occurrence (or selection, or use) decreases (negative coefficient) with increasing distance from the feature. Therefore, the negative coefficient estimated for distance to fence indicates selection for proximity to fences, which is consistent with what we know about the ecology of the species. sd(LOSHdata$FenceDist) ## [1] 383 If we increase the distance from a fence by 382m, logit(\\(\\pi\\)) will decrease by 0.756. Exponentiating the partial slope produces an odds ratio that is easier to interpret. exp(-0.75893) ## [1] 0.468 This means the probability of presence is 0.468 less likely than absence with an increase of 382m to the nearest fence. It is important to remember that all binary GLMs predict a linear relation in the scale of the logit link function, which become logistic when the function is inverted to compare model predictions with the data. The non-linearity of the inverted link function mean the slopes can no longer be interpreted as the change in the mean of the response variable per unit change of the explanatory variable. This is because the central range of the logistic curve is steeper than the tails (which are flatter). Therefore any unit change of an explanatory variable in the midpoint values will result in a larger change in the probability of presence than when the change occurs at the tail ends of the range. There is a rule-of-thumb to interpret the values in logistic regression in terms of probabilities (not logits or odds ratios). For example, distance to fence coefficient estimate -0.75893/4 = -0.19 means that a change of one standard deviation (382m) would produce a 19% reduction in the probability of presence. This estimate represents the midpoint values and therefore changes elsewhere in the range would likely entail smaller changes in the probability of presence. 8.3.4 Plot Coefficients It is always useful to plot the coefficient estimates and their associated likelihood confidence intervals. This type of figure is one of the key results that you should present in any manuscript presenting this type of model. There are, of course, many ways to do this. Inchausti provides an excellent example using broom package. require(broom) tidy(m1, conf.int=T) ## # A tibble: 6 × 7 ## term estimate std.error statistic p.value conf.low conf.high ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) -3.43 0.335 -10.2 1.33e-24 -4.18 -2.85 ## 2 StSageHgtSD -0.602 0.362 -1.66 9.67e- 2 -1.33 0.0937 ## 3 StHerb -0.781 0.415 -1.88 5.98e- 2 -1.67 -0.0512 ## 4 StSage 0.184 0.299 0.614 5.39e- 1 -0.408 0.770 ## 5 StRough200m 0.0780 0.266 0.293 7.70e- 1 -0.493 0.565 ## 6 StFenceDist -0.759 0.352 -2.15 3.12e- 2 -1.53 -0.139 param.m1 &lt;- tidy(m1, conf.int=T) # save tibble # plot coefficient estimates and confidence intervals ggplot(param.m1, aes(x=term, y=estimate)) + theme_bw() + geom_point(col=&quot;black&quot;, size=2) + geom_linerange(aes(ymin = conf.low, ymax = conf.high)) + coord_flip() + labs(x = &quot;Parameter&quot;) + theme (axis.title = element_text(size = 12), axis.text = element_text(size = 10), axis.title.y = element_blank()) 8.3.5 Plot Model Predictions One of the most informative way to present your model results is using figures presenting the predicted probabilities. This can be accomplished using the ggpredict function from the ggeffects package. These plots are often referred to as “conditional plots” or “marginal effects plots”. Essentially, the plots represent the predicted values over the range of a covariate while holding all other covariates in the model at their means. The ggpredict command uses the inverse of the logit link function to generate the curves using the parameter estimates. To create the plots we need to generate the predicted values for each covariate that we want to plot. For this example we will plot two the variables above that have confidence intervals that do not overlap zero. # Generate predicted values pred.m1.StHerb=ggpredict(m1, terms = c(&quot;StHerb [all]&quot;)) pred.m1.StFenceDist=ggpredict(m1, terms = c(&quot;StFenceDist [all]&quot;)) # Plot predictions plot.m1.StHerb=plot(pred.m1.StHerb, colors=&quot;greyscale&quot;) + labs (x=&quot;Percent Herbaceous cover (std)&quot;, y = &quot;Probability of presence&quot;) + theme(plot.title=element_blank(), axis.title.y=element_blank(), axis.title.x=element_text(size=10), axis.text=element_text(size=8)) plot.m1.StFenceDist=plot(pred.m1.StFenceDist, colors=&quot;greyscale&quot;) + labs (x=&quot;Distance to Fence (std)&quot;, y = &quot;Probability of presence&quot;) + theme(plot.title=element_blank(), axis.title.y=element_blank(), axis.title.x=element_text(size=10), axis.text=element_text(size=8)) plot_grid(plot.m1.StHerb, plot.m1.StFenceDist, ncol=1, labels=c(&#39;A&#39;, &#39;B&#39;), align=&quot;v&quot;, label_x=0.95) 8.3.6 Model Assessment Model assessment through inspection of the model residuals is not as straight forward when a link function is involved. With a general linear model we could investigate model fit with plot(modelname). Try that with this model and take a look at the figures produced by entering plot(m1) into your R console. The figures look strange. In order to assess model fit in a logistic regression several other steps are required and make use of randomized quantile residuals. These values are estimated using simulated data from the fitted model to generate a cumulative density function of the response variable. Many more details and specific equations are presented in Chapter 8 of the Inchausti book. For our purposes, we can leave the math to those experts with maths degrees and focus on the general steps. We will use the package DHARMa to generate randomized quantile residuals for our GLMs. The process involves three steps. First, simulate new dat sets from the fitted GLM. These simulations then allow for the estimate of the cumulative distribution function of each observed value of the response variable for the simulated data set. Finally, the package calculates the randomized quantile residual of each observed value of the response variable. These data are then used to produce residual plots analogous to those we used in the last chapter in the Model Validation section. resid.m1 &lt;- broom::augment(m1) # store residuals # generate randomized quantiles res.m1 &lt;- simulateResiduals(fittedModel = m1, n = 1e3, integerResponse = T, refit = F, plot = F) # replace default deviance residuals with simulated resid.m1$.std.resid = residuals(res.m1, quantileFunction = qnorm) # generate plots for residual fit. qq.m1 &lt;- ggplot(data=resid.m1, aes(sample = .std.resid)) + geom_qq(distribution = qnorm, size = 1) + theme_bw() + stat_qq_line() + stat_qq_band(alpha=0.3) + labs(x=&quot;Theoretical Quantiles&quot;, y = &quot;Sample Quantiles&quot;) + theme(axis.title=element_text(size=12), axis.text=element_text(size=10)) res.fit.m1 &lt;- ggplot(data=resid.m1, aes(x =.fitted, y=.std.resid)) + geom_point(col=&quot;black&quot;, size=1) + geom_hline(yintercept=0, linetype = 2, linewidth = 1) + theme_bw() + labs(x = &quot;Fitted&quot;, y=&quot;Residuals&quot;) + theme(axis.title=element_text(size=12), axis.text=element_text(size=10)) res.Herb.m1 &lt;- ggplot(data=resid.m1, aes(x=StHerb, y=.std.resid)) + geom_point(col=&quot;black&quot;, size=1) + geom_hline(yintercept = 0, linetype=2, linewidth=1) + theme_bw() + labs(x=&quot;Herb (std)&quot;, y=&quot;Residuals&quot;) + theme(axis.title.y=element_blank(), axis.text = element_text(size=12), axis.title.x=element_text(size=10)) res.FenceDist.m1 &lt;- ggplot(data=resid.m1, aes(x=StFenceDist, y=.std.resid)) + geom_point(col=&quot;black&quot;, size=1) + geom_hline(yintercept = 0, linetype=2, linewidth=1) + theme_bw() + labs(x=&quot;Herb (std)&quot;, y=&quot;Residuals&quot;) + theme(axis.title.y=element_blank(), axis.text = element_text(size=12), axis.title.x=element_text(size=10)) Cook.m1 &lt;- ggplot(data=resid.m1, aes(x=1:nrow(resid.m1), y=.cooksd))+ geom_linerange(aes(ymin=0, ymax=.cooksd))+ theme_bw() + labs(x=&quot;Order of data&quot;, y=&quot;Cook&#39;s distance&quot;) + theme(axis.title=element_text(size=12), axis.text = element_text(size=10)) plot_grid(qq.m1, res.fit.m1, res.Herb.m1, res.FenceDist.m1, Cook.m1, ncol=2, labels = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;, &quot;D&quot;, &quot;E&quot;), align = &quot;hv&quot;, label_x = 0.85, label_y = 0.95) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
